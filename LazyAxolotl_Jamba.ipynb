{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/Awesome-Fine-tuning/blob/main/LazyAxolotl_Jamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶é LazyAxolotl - Jamba\n",
        "\n",
        "> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "This version of [LazyAxolotl](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW) is dedicated to fine-tuning [ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1). It uses the following version of [lazyaxolotl.sh](https://gist.githubusercontent.com/mlabonne/e9f53fca97d9edb7aca0717770185438) and has successfully fine-tuned several models, including [mlabonne/Jambatypus-v0.1](https://huggingface.co/mlabonne/Jambatypus-v0.1).\n",
        "\n",
        "*Thanks to [Wing Lian](https://github.com/winglian) for making Axolotl and for his quick implementation of Jamba.*\n",
        "\n",
        "‚ö†Ô∏è I recommende using 2xA100 80GB or equivalent to fine-tune this model. With a single A100 80GB, I wasn't able to merge the adapter in FP16. If you want to still use a single GPU, it will merge in 8-bit precision instead."
      ],
      "metadata": {
        "id": "1Wq4SB9A_9ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"Jambatypus-v0.1\"\n",
        "yaml_config = f\"\"\"\n",
        "base_model: ai21labs/Jamba-v0.1\n",
        "trust_remote_code: true\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: chargoddard/Open-Platypus-Chat\n",
        "    type: sharegpt\n",
        "chat_template: chatml\n",
        "dataset_prepared_path:\n",
        "val_set_size: 0.01\n",
        "output_dir: ./out\n",
        "\n",
        "sequence_len: 4096\n",
        "sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "eval_sample_packing: false\n",
        "\n",
        "use_wandb: true\n",
        "wandb_project: axolotl\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name: {MODEL}\n",
        "wandb_log_model:\n",
        "\n",
        "adapter: qlora\n",
        "lora_r: 32\n",
        "lora_alpha: 64\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "\n",
        "low_cpu_mem_usage: true\n",
        "gradient_accumulation_steps: 8\n",
        "micro_batch_size: 1\n",
        "num_epochs: 1\n",
        "optimizer: adamw_bnb_8bit\n",
        "adam_beta2: 0.95\n",
        "adam_epsilon: 0.00001\n",
        "max_grad_norm: 1.0\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "gradient_checkpointing_kwargs:\n",
        "  use_reentrant: false\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch: 4\n",
        "saves_per_epoch: 4\n",
        "save_total_limit: 2\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "special_tokens:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import runpod\n",
        "except ImportError:\n",
        "  !pip install -qqq runpod --progress-bar off\n",
        "\n",
        "import runpod\n",
        "from runpod.error import QueryError\n",
        "from google.colab import userdata, runtime\n",
        "import requests\n",
        "import json\n",
        "import yaml\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def upload_gist(text, gist_name, gh_token, gist_description=\"\"):\n",
        "    gist_content = {\n",
        "        \"description\": gist_description,\n",
        "        \"public\": False,\n",
        "        \"files\": {\n",
        "            f\"{gist_name}\": {\n",
        "                \"content\": text\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Headers for the request\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {gh_token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.post(\n",
        "        \"https://api.github.com/gists\", headers=headers, json=gist_content\n",
        "    )\n",
        "\n",
        "    if response.status_code == 201:\n",
        "        gist_data = response.json()\n",
        "        raw_url = gist_data['files'][gist_name]['raw_url']\n",
        "        print(f\"Uploaded Axolotl config as gist: {raw_url}\")\n",
        "        return raw_url\n",
        "    else:\n",
        "        print(\n",
        "            f\"Failed to upload gist. Status code: {response.status_code}. Response: {response.text}\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "def delete_gist(gh_token, gist_id):\n",
        "    # Headers for the request\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {gh_token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.delete(\n",
        "        f\"https://api.github.com/gists/{gist_id}\", headers=headers\n",
        "    )\n",
        "\n",
        "    if response.status_code == 204:\n",
        "        print(\"Gist has been deleted.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\n",
        "            f\"Failed to delete gist. Status code: {response.status_code}. Response: {response.text}\"\n",
        "        )\n",
        "        return False\n",
        "\n",
        "# @title ## Training parameters\n",
        "GPU = \"NVIDIA A100 80GB PCIe\" # @param [\"NVIDIA A100 80GB PCIe\", \"NVIDIA A100-SXM4-80GB\", \"NVIDIA A30\", \"NVIDIA A40\", \"NVIDIA GeForce RTX 3070\", \"NVIDIA GeForce RTX 3080\", \"NVIDIA GeForce RTX 3080 Ti\", \"NVIDIA GeForce RTX 3090\", \"NVIDIA GeForce RTX 3090 Ti\", \"NVIDIA GeForce RTX 4070 Ti\", \"NVIDIA GeForce RTX 4080\", \"NVIDIA GeForce RTX 4090\", \"NVIDIA H100 80GB HBM3\", \"NVIDIA H100 PCIe\", \"NVIDIA L4\", \"NVIDIA L40\", \"NVIDIA RTX 4000 Ada Generation\", \"NVIDIA RTX 4000 SFF Ada Generation\", \"NVIDIA RTX 5000 Ada Generation\", \"NVIDIA RTX 6000 Ada Generation\", \"NVIDIA RTX A2000\", \"NVIDIA RTX A4000\", \"NVIDIA RTX A4500\", \"NVIDIA RTX A5000\", \"NVIDIA RTX A6000\", \"Tesla V100-FHHL-16GB\", \"Tesla V100-PCIE-16GB\", \"Tesla V100-SXM2-16GB\", \"Tesla V100-SXM2-32GB\"]\n",
        "NUMBER_OF_GPUS = 2 # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "CONTAINER_DISK = 300 # @param {type:\"slider\", min:50, max:500, step:25}\n",
        "CLOUD_TYPE = \"SECURE\" # @param [\"COMMUNITY\", \"SECURE\"]\n",
        "SCRIPT = \"https://gist.githubusercontent.com/mlabonne/a73b7d543f49b7ea2b52068414304414/raw\" # @param {type:\"string\"}\n",
        "ZERO = \"Stage 2\" # @param [\"None\", \"Stage 2\"]\n",
        "LLM_AUTOEVAL = False # @param {type:\"boolean\"}\n",
        "DEBUG = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## Tokens\n",
        "# @markdown Enter the name of your tokens in the Secrets tab.\n",
        "USERNAME = \"mlabonne\" # @param {type:\"string\"}\n",
        "RUNPOD_TOKEN = \"runpod\" # @param {type:\"string\"}\n",
        "HUGGING_FACE_TOKEN = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "WANDB_TOKEN = \"wandb\" # @param {type:\"string\"}\n",
        "GITHUB_TOKEN = \"github\" # @param {type:\"string\"}\n",
        "\n",
        "# Environment variables\n",
        "runpod.api_key = userdata.get(RUNPOD_TOKEN)\n",
        "WANDB_API_KEY = userdata.get(WANDB_TOKEN)\n",
        "HF_TOKEN = userdata.get(HUGGING_FACE_TOKEN)\n",
        "GITHUB_API_TOKEN = userdata.get(GITHUB_TOKEN)\n",
        "\n",
        "# Make sure it's a valid YAML file\n",
        "config = yaml.safe_load(yaml_config)\n",
        "\n",
        "# Upload the YAML file to GitHub\n",
        "gist_url = upload_gist(yaml_config, \"config.yaml\", GITHUB_API_TOKEN, f\"{MODEL} - https://huggingface.co/{USERNAME}/{MODEL}\")\n",
        "\n",
        "# Summary\n",
        "base_model = config.get('base_model', 'Unknown model')\n",
        "dataset_info = []\n",
        "datasets = config.get('datasets', [])\n",
        "for dataset in datasets:\n",
        "    path = dataset.get('path', 'Unknown path')\n",
        "    dtype = dataset.get('type', 'Unknown type')\n",
        "    dataset_info.append(f\"{path} ({dtype})\")\n",
        "datasets_summary = ', '.join(dataset_info)\n",
        "\n",
        "# Create a pod\n",
        "keep_trying = True\n",
        "try:\n",
        "    while keep_trying:\n",
        "        try:\n",
        "            pod = runpod.create_pod(\n",
        "                name=f\"LazyAxolotl - {MODEL}\",\n",
        "                image_name=\"winglian/axolotl-cloud:main-latest\",\n",
        "                gpu_type_id=GPU,\n",
        "                cloud_type=CLOUD_TYPE,\n",
        "                gpu_count=NUMBER_OF_GPUS,\n",
        "                volume_in_gb=0,\n",
        "                container_disk_in_gb=CONTAINER_DISK,\n",
        "                template_id=\"eul6o46pab\",\n",
        "                env={\n",
        "                    \"HF_TOKEN\": HF_TOKEN,\n",
        "                    \"SCRIPT\": SCRIPT,\n",
        "                    \"WANDB_API_KEY\": WANDB_API_KEY,\n",
        "                    \"GIST_URL\": gist_url,\n",
        "                    \"MODEL_NAME\": MODEL,\n",
        "                    \"BASE_MODEL\": config['base_model'],\n",
        "                    \"USERNAME\": USERNAME,\n",
        "                    \"ZERO\": ZERO,\n",
        "                    \"LLM_AUTOEVAL\": LLM_AUTOEVAL,\n",
        "                    \"MODEL_ID\": USERNAME + \"/\" + MODEL,\n",
        "                    \"BENCHMARK\": \"nous\",\n",
        "                    \"REPO\": \"https://github.com/mlabonne/llm-autoeval.git\",\n",
        "                    \"TRUST_REMOTE_CODE\": True,\n",
        "                    \"GITHUB_API_TOKEN\": GITHUB_API_TOKEN,\n",
        "                    \"DEBUG\": DEBUG,\n",
        "                }\n",
        "            )\n",
        "            print(f\"This runs trains {base_model} on {datasets_summary}.\")\n",
        "            print(\"https://www.runpod.io/console/pods\")\n",
        "            keep_trying = False\n",
        "        except QueryError as e:\n",
        "            print(f\"\\033[31m‚ö†Ô∏è ERROR: The requested pod ({NUMBER_OF_GPUS}x {GPU} with {CONTAINER_DISK} GB) is not currently available. Trying again in 30 seconds...\\033[39m\")\n",
        "            time.sleep(30)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"KeyboardInterrupt detected, cleaning up before stopping...\")\n",
        "    delete_gist(GITHUB_API_TOKEN, gist_url.split('/')[4])"
      ],
      "metadata": {
        "id": "d5mYzDo1q96y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6eea1e4-be1f-4de5-b251-2aada1663550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded Axolotl config as gist: https://gist.githubusercontent.com/mlabonne/9b239b771be74fadc4f0e10a769a9eff/raw/552c1dc5598de4db756a02634ea7ac061382fe94/config.yaml\n",
            "This runs trains ai21labs/Jamba-v0.1 on mhenrichsen/alpaca_2k_test (alpaca).\n",
            "https://www.runpod.io/console/pods\n"
          ]
        }
      ]
    }
  ]
}