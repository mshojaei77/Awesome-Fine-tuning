{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/Awesome-Fine-tuning/blob/main/%E2%9A%A1Online_DPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Online DPO with TRL"
      ],
      "metadata": {
        "id": "hiQILPUKXmCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIE8rtfFZTrd",
        "outputId": "b9a75e7e-cfe1-4fa0-b1da-4fbbca37531e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'trl'...\n",
            "remote: Enumerating objects: 8567, done.\u001b[K\n",
            "remote: Counting objects: 100% (1669/1669), done.\u001b[K\n",
            "remote: Compressing objects: 100% (423/423), done.\u001b[K\n",
            "remote: Total 8567 (delta 1471), reused 1344 (delta 1231), pack-reused 6898\u001b[K\n",
            "Receiving objects: 100% (8567/8567), 7.08 MiB | 23.76 MiB/s, done.\n",
            "Resolving deltas: 100% (5926/5926), done.\n"
          ]
        }
      ],
      "source": [
        "# clone trl at oline-dpo-llmjudge branch\n",
        "!pip install git+https://github.com/huggingface/trl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "from trl import HfPairwiseJudge, ModelConfig\n",
        "from trl.commands.cli_utils import TrlParser\n",
        "from trl.trainer import OnlineDPOConfig, OnlineDPOTrainer\n",
        "from trl.trainer.utils import SIMPLE_QUERY_CHAT_TEMPLATE"
      ],
      "metadata": {
        "id": "hOFyF1r3r69p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Prepare the dataset for online DPO\n",
        "\n",
        "### Section 1: Prepare Dataset\n",
        "\n",
        "Preparing the dataset is a crucial step in the training process, ensuring that the model receives input data in a format that is optimized for efficient learning. This section describes how to pre-process and tokenize the dataset to be used for Online Direct Preference Optimization (ODPO).\n",
        "\n",
        "1. **Load the Dataset**: Start by loading the dataset that contains the prompts and responses which will be used for training the model. In the provided code, this is done using the `load_dataset` function, which retrieves the dataset specified by `dataset_name` in the `ScriptArguments` class.\n",
        "\n",
        "2. **Sanity Check**: If you are conducting a sanity check, the code limits the size of the dataset to the first 1024 entries. This step helps to quickly validate the training pipeline without running the full dataset, making the debugging process faster and easier.\n",
        "\n",
        "3. **Tokenization**: The dataset must be tokenized before training. Tokenization is the process of converting text into a sequence of tokens that the model can understand. In the `prepare_dataset` function, each entry in the dataset is tokenized using the provided `tokenizer`. The key parameters include:\n",
        "   - **Input Text Field**: The specific field in the dataset containing the text to be tokenized, specified by `dataset_text_field`.\n",
        "   - **Padding**: The function disables padding during tokenization to avoid unnecessary processing, as padding can be applied later during collation if needed.\n",
        "   - **Multiprocessing**: Tokenization is parallelized using multiple processes (`num_proc=4`) to speed up the process.\n",
        "\n",
        "   This pre-tokenization approach ensures that the data is efficiently processed and ready for training, avoiding repeated tokenization during each training step.\n",
        "\n",
        "4. **Prepare Train and Evaluation Sets**:\n",
        "   - **Training Dataset**: The training split of the dataset, specified by `dataset_train_split`, is prepared by applying the `prepare_dataset` function to it. This results in a tokenized version of the training data that the model will use to learn the alignment.\n",
        "   - **Evaluation Dataset**: If a validation or test split is provided (`dataset_test_split`), it is similarly prepared using the same tokenization function. This allows the model’s performance to be evaluated on a separate set of data, ensuring that it generalizes well beyond the training examples.\n",
        "\n",
        "By following these steps, the dataset is transformed into a format that is optimized for the model training process. This preparation not only speeds up the training process but also ensures that the model receives consistent and well-structured input data, crucial for effective learning and alignment."
      ],
      "metadata": {
        "id": "ZN03-I0VYd1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# Dataset\n",
        "################\n",
        "\n",
        "def prepare_dataset(dataset, tokenizer, dataset_text_field):\n",
        "    \"\"\"pre-tokenize the dataset before training; only collate during training\"\"\"\n",
        "\n",
        "    def tokenize(element):\n",
        "        outputs = tokenizer(\n",
        "            element[dataset_text_field],\n",
        "            padding=False,\n",
        "        )\n",
        "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
        "\n",
        "    return dataset.map(\n",
        "        tokenize,\n",
        "        remove_columns=dataset.column_names,\n",
        "        batched=True,\n",
        "        num_proc=4,  # multiprocessing.cpu_count(),\n",
        "        load_from_cache_file=False,\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    dataset_name:str=\"trl-internal-testing/tldr-preference-sft-trl-style\"\n",
        "    dataset_text_field: str = \"prompt\"\n",
        "    dataset_train_split: str = \"train\"\n",
        "    dataset_test_split: Optional[str] = \"validation\"\n",
        "    max_length: int = 512\n",
        "    sanity_check: bool=True\n",
        "    response_length: int = 53\n",
        "    stop_token: str = \"eos\"\n",
        "    non_eos_penalty: bool = False\n",
        "\n",
        "args = ScriptArguments()\n",
        "\n",
        "\n",
        "raw_datasets = load_dataset(args.dataset_name)\n",
        "if args.sanity_check:\n",
        "    for key in raw_datasets:\n",
        "        raw_datasets[key] = raw_datasets[key].select(range(1024))\n",
        "train_dataset = raw_datasets[args.dataset_train_split]\n",
        "train_dataset = prepare_dataset(train_dataset, tokenizer, args.dataset_text_field)\n",
        "\n",
        "if args.dataset_test_split is not None:\n",
        "    eval_dataset = raw_datasets[args.dataset_test_split]\n",
        "    eval_dataset = prepare_dataset(eval_dataset, tokenizer, args.dataset_text_field)\n",
        "else:\n",
        "    eval_dataset = None"
      ],
      "metadata": {
        "id": "RGezhGdnsM_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define the model and Tokenizer\n",
        "\n",
        "### Section 2: Prepare Model\n",
        "\n",
        "Once the dataset is ready, the next step is to set up the model and tokenizer for training with Online Direct Preference Optimization (ODPO). The provided code outlines how to configure and prepare the necessary components to effectively implement ODPO.\n",
        "\n",
        "1. **Configure Model and Tokenizer**:\n",
        "   - **Model Configuration**: Begin by defining the configuration for ODPO using the `OnlineDPOConfig` class. This includes specifying paths to the pre-trained models (`sft_model_path` and `reward_model_path`), the output directory for saving the trained models, learning rate, batch size, and the total number of training episodes.\n",
        "   - **Tokenizer Setup**: The tokenizer is initialized using `AutoTokenizer.from_pretrained`, which loads the tokenizer associated with the pre-trained model path defined in `ModelConfig`. Special tokens, like the padding token, are added to ensure the tokenizer handles inputs correctly. If a chat template is not provided, it defaults to `SIMPLE_QUERY_CHAT_TEMPLATE`, ensuring consistent input formatting.\n",
        "\n",
        "2. **Load Pre-trained Models**:\n",
        "   - **Language Model**: Load the pre-trained language model using `AutoModelForCausalLM.from_pretrained`, which retrieves the model specified in `model_config.model_name_or_path`. This model will serve as both the base model and the reference model (`ref_model`), which is essential for comparing outputs during ODPO training.\n",
        "   - **Reward Model**: If a reward model path is provided, load it using `AutoModelForSequenceClassification.from_pretrained`, specifying `num_labels=1` since it outputs a single scalar value representing the reward. This reward model will be used to evaluate the quality of generated responses during training.\n",
        "   - **Judge Model**: If the configuration includes a judge model, instantiate it using `HfPairwiseJudge`. This model will be responsible for providing pairwise comparisons of generated responses, which are crucial for aligning the model during ODPO training.\n",
        "\n",
        "3. **Model Preparation**:\n",
        "   - **Pre-trained Model Selection**: The code selects `EleutherAI/pythia-14m` as the pre-trained model, which is a lightweight and efficient model suitable for experimentation and alignment tasks. Both the main model and the reference model are initialized with this pre-trained base.\n",
        "   - **Reward Model Integration**: The reward model is integrated into the training loop to evaluate the generated responses and guide the alignment process. If no specific reward model is provided, this step can be skipped, but it is crucial for ensuring the model learns to produce aligned outputs.\n",
        "   - **Judge Model Integration**: The judge model is optionally integrated, providing real-time feedback on the quality of outputs during training. This model helps in making fine-grained adjustments to the main model’s parameters based on the pairwise comparison of outputs.\n",
        "\n",
        "By following these steps, the model and tokenizer are properly configured and ready for the ODPO training phase. This setup ensures that the model is capable of receiving and processing feedback, which is essential for iteratively improving alignment throughout the training process."
      ],
      "metadata": {
        "id": "pfCqILNCYiCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# Model & Tokenizer\n",
        "################\n",
        "\n",
        "config = OnlineDPOConfig(\n",
        "    sft_model_path=\"EleutherAI/pythia-14m\",\n",
        "    reward_model_path=\"EleutherAI/pythia-14m\",\n",
        "    output_dir=\"models/minimal/online_dpo_llmjudge\",\n",
        "    learning_rate=3e-6,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=64,\n",
        "    total_episodes=30000,\n",
        ")\n",
        "\n",
        "model_config = ModelConfig(\n",
        "    model_name_or_path=\"EleutherAI/pythia-14m\",\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_config.model_name_or_path,\n",
        "    padding_side=\"left\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = SIMPLE_QUERY_CHAT_TEMPLATE\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_config.model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_config.model_name_or_path)\n",
        "\n",
        "if config.reward_model_path is not None:\n",
        "    reward_model = AutoModelForSequenceClassification.from_pretrained(config.reward_model_path, num_labels=1)\n",
        "else:\n",
        "    reward_model = None\n",
        "\n",
        "if config.judge is not None:\n",
        "    judge = HfPairwiseJudge()\n",
        "else:\n",
        "    judge = None\n"
      ],
      "metadata": {
        "id": "saCXGXhnYWBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Train the model\n",
        "\n",
        "### Section 3: Training\n",
        "\n",
        "With the dataset and model prepared, the training phase can now commence. This phase involves leveraging the Online Direct Preference Optimization (ODPO) process to iteratively improve the alignment of the model based on the feedback from the reward and judge models.\n",
        "\n",
        "1. **Initialize Training**:\n",
        "   - The training process begins by setting up the `OnlineDPOTrainer` with the prepared model, reference model, reward model, and judge model. The `trainer` is configured with the provided `config` settings, which include important parameters like learning rate, batch size, and the number of training episodes.\n",
        "   - The training and evaluation datasets, which have been pre-tokenized and processed, are also passed to the trainer. The tokenizer is included to ensure that any additional text processing during training is handled consistently.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - The `trainer.train()` method initiates the ODPO training loop. During each iteration, the model generates responses based on the prompts from the training dataset.\n",
        "   - These generated responses are compared against the reference model’s responses. The judge model evaluates which response is preferred, providing critical feedback for aligning the model's outputs with desired behaviors.\n",
        "\n",
        "3. **Model Updates**:\n",
        "   - The feedback from the judge and reward models is used to update the model’s parameters. The ODPO objective function guides this update, optimizing the model's policy to produce outputs that are increasingly aligned with the annotated preferences.\n",
        "   - The reference model (`ref_model`) plays a crucial role in stabilizing the updates by serving as a baseline for comparison, ensuring that the main model does not drift too far from its original performance.\n",
        "\n",
        "4. **Iterate**:\n",
        "   - The process of generating responses, receiving feedback, and updating the model is repeated for the specified number of episodes (`total_episodes`). Each iteration helps the model gradually refine its ability to generate preferred responses, improving its alignment over time.\n",
        "\n",
        "5. **Monitor and Adjust**:\n",
        "   - Throughout the training process, it’s important to monitor the model’s performance. This can be done by evaluating metrics such as the win rates of the preferred responses. Monitoring ensures that the model continues to improve and does not overfit or diverge from the desired output quality.\n",
        "   - If necessary, adjustments to the training parameters or model configuration can be made mid-training to better align the model's performance with the desired outcomes.\n",
        "\n",
        "6. **Evaluate and Fine-tune**:\n",
        "   - After the training loop completes, the model should be evaluated on the separate validation dataset (`eval_dataset`). This evaluation helps determine how well the model generalizes to new prompts and whether it meets the alignment goals.\n",
        "   - Depending on the evaluation results, fine-tuning might be required to address any remaining issues or to further refine specific aspects of the model’s behavior.\n",
        "\n",
        "By following these steps, the model undergoes a thorough and iterative training process, ensuring that it aligns more closely with human preferences and generates reliable, high-quality responses. The use of the `OnlineDPOTrainer` streamlines this process, allowing for effective integration of feedback and continual model improvement."
      ],
      "metadata": {
        "id": "U7btGQxYYmR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# Training\n",
        "################\n",
        "\n",
        "trainer = OnlineDPOTrainer(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    ref_model=ref_model,\n",
        "    reward_model=reward_model,\n",
        "    judge=judge,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5HS_DP1LwE8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}