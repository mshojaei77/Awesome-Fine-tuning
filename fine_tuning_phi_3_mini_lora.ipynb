{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsi3Q1355PgWlBdl/BdlGa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/Awesome-Fine-tuning/blob/main/fine_tuning_phi_3_mini_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction Tuning a Phi-3-mini Model for Persian Question Answering with LoRA\n",
        "\n",
        "This tutorial provides a step-by-step guide on fine-tuning a Phi-3-mini model for Persian question answering using instruction tuning with LoRA (Low-Rank Adaptation) on the Hugging Face Hub. We'll utilize the `mshojaei77/merged_persian_qa` dataset containing over 527k question-answer pairs for this purpose.\n",
        "\n",
        "### 1. Setup and Installation\n",
        "\n",
        "Let's begin by setting up our environment and installing the necessary libraries."
      ],
      "metadata": {
        "id": "gh4BsXTgA___"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJjBPMP4_SFz"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq --upgrade bitsandbytes transformers peft accelerate datasets trl flash_attn wandb huggingface_hub python-dotenv absl-py nltk rouge_score\n",
        "\n",
        "!pip list | grep transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installs essential libraries like `transformers`, `peft` (for LoRA), `datasets`, `trl` (for SFTTrainer), and others.\n",
        "\n",
        "Now, import the required modules:"
      ],
      "metadata": {
        "id": "BWKhGDuoBDLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    pipeline\n",
        ")\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "3QJ99WKJA55C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Data Preparation\n",
        "\n",
        "Download the `mshojaei77/merged_persian_qa` dataset and examine its structure."
      ],
      "metadata": {
        "id": "9ERcy9tNdvQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mshojaei77/merged_persian_qa\"\n",
        "dataset_split = \"train\"\n",
        "dataset = load_dataset(dataset_name, split=dataset_split)\n",
        "\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(dataset[randrange(len(dataset))])\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "GPSAEIQcFQZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset should have two columns: \"Context\" for the question and \"Response\" for the answer.\n",
        "\n",
        "### 3. Model and Tokenizer\n",
        "\n",
        "We'll use the `microsoft/Phi-3-mini-4k-instruct` model for this tutorial. You can choose a different Phi-3 variant based on your requirements."
      ],
      "metadata": {
        "id": "nDzGKqsFd0qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_id, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\",\n",
        "          attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"sdpa\"\n",
        ")"
      ],
      "metadata": {
        "id": "GJszSwGg2Wm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code loads the tokenizer and the model. Ensure you have a GPU available; otherwise, adjust the `device_map` parameter accordingly.\n",
        "\n",
        "### 4. Instruction Tuning Format\n",
        "\n",
        "We'll format our data for instruction tuning by creating a \"messages\" column using the ChatML format."
      ],
      "metadata": {
        "id": "CL-2JkDid42x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_message_column(row):\n",
        "    return {\"messages\": [\n",
        "        {\"role\": \"user\", \"content\": row['Context']},\n",
        "        {\"role\": \"assistant\", \"content\": row['Response']}\n",
        "    ]}\n",
        "\n",
        "def format_dataset_chatml(row):\n",
        "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n",
        "\n",
        "dataset_chatml = dataset.map(create_message_column)\n",
        "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
        "\n",
        "print(dataset_chatml[0])"
      ],
      "metadata": {
        "id": "F6oF7NfZcwC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This formats the dataset to have a conversational structure with user instructions and assistant responses.\n",
        "\n",
        "### 5. Training Setup\n",
        "\n",
        "Now, we'll split the dataset, define our training arguments, and configure the LoRA parameters."
      ],
      "metadata": {
        "id": "cPiAtMTyd6Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
        "\n",
        "lora_r = 16\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./phi-3-mini-LoRA-persian-qa\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=100,\n",
        "    eval_steps=100,\n",
        "    report_to=\"wandb\",\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "uMwSt5gAdJM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines parameters for training, evaluation, and LoRA. Adjust these based on your hardware and desired performance.\n",
        "\n",
        "### 6. Training with SFTTrainer\n",
        "\n",
        "We'll utilize the `SFTTrainer` from the `trl` library for efficient instruction tuning."
      ],
      "metadata": {
        "id": "RvgRcCd_eAr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset_chatml['train'],\n",
        "    eval_dataset=dataset_chatml['test'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "bs35NC1XdNcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the model using the specified parameters and saves the trained model locally.\n",
        "\n",
        "### 7. Evaluation\n",
        "\n",
        "We can assess the fine-tuned model's performance using the ROUGE metric."
      ],
      "metadata": {
        "id": "hsK_M39veClp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)\n",
        "\n",
        "def calculate_rouge(row):\n",
        "    response = test_inference(row['messages'][0]['content'])\n",
        "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    result['response'] = response\n",
        "    return result\n",
        "\n",
        "metricas = dataset_chatml['test'].select(range(0, 500)).map(calculate_rouge, batched=False)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"Rouge 1 Mean: \", np.mean(metricas['rouge1']))\n",
        "print(\"Rouge 2 Mean: \", np.mean(metricas['rouge2']))\n",
        "print(\"Rouge L Mean: \", np.mean(metricas['rougeL']))\n",
        "print(\"Rouge Lsum Mean: \", np.mean(metricas['rougeLsum']))"
      ],
      "metadata": {
        "id": "vTUH2owOdOPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates and prints the average ROUGE scores for the generated responses compared to the ground truth answers.\n",
        "\n",
        "### 8. Saving and Sharing\n",
        "\n",
        "Finally, save your fine-tuned model and tokenizer to the Hugging Face Hub for sharing and future use.\n"
      ],
      "metadata": {
        "id": "yoHzgrckeI2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    args.output_dir,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "merged_model = new_model.merge_and_unload()\n",
        "\n",
        "merged_model.save_pretrained(\"merged_model\", trust_remote_code=True, safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "\n",
        "hf_model_repo = \"your-username/your-model-name\"  # Replace with your desired repo name\n",
        "\n",
        "merged_model.push_to_hub(hf_model_repo)\n",
        "tokenizer.push_to_hub(hf_model_repo)"
      ],
      "metadata": {
        "id": "4UoSLZSPdT4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Congratulations! You have successfully fine-tuned a Phi-3-mini model for Persian question answering using instruction tuning and LoRA. You can now utilize this fine-tuned model for various downstream tasks involving Persian question answering. Remember to replace placeholder values like your Hugging Face username and desired model repository names throughout the code.\n"
      ],
      "metadata": {
        "id": "RrxZXOkRePav"
      }
    }
  ]
}