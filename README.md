# Awesome-Fine-tuning

A curated list of resources for fine-tuning large language models (LLMs). Contribute to enhance LLM performance and explore various fine-tuning methods with our community!

This repository aims to provide a comprehensive and easily navigable list of fine-tuning resources for popular LLMs. The list below is a starting point, and we encourage contributions to make it even more valuable for the community.

## Fine-tuning Resources

*   **Model:** `qwen_grpo_training.ipynb` (**Gemma**)
    *   **Method:** GRPO Training
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [Copy_of_qwen_grpo_training.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Copy_of_qwen_grpo_training.ipynb)
    *   **Description:** Notebook for Qwen GRPO (Gradient Ratio Policy Optimization) training adapted for Gemma.

*   **Model:** `Deepseek_Llava_VLM_trl.ipynb` (**Llama**)
    *   **Method:** TRL
    *   **Library:**
    *   **Dataset Type:** VLM
    *   **Notebook:** [Deepseek_Llava_VLM_trl.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Deepseek_Llava_VLM_trl.ipynb)
    *   **Description:** Fine-tuning Deepseek-Llava VLM using Transformer Reinforcement Learning (TRL).

*   **Model:** `Fine_tune_LLMs_with_Axolotl.ipynb` (**Yi**)
    *   **Method:** Axolotl
    *   **Library:** Axolotl
    *   **Dataset Type:** General
    *   **Notebook:** [Fine_tune_LLMs_with_Axolotl.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Fine_tune_LLMs_with_Axolotl.ipynb)
    *   **Description:** General notebook for fine-tuning LLMs using Axolotl library, demonstrated with Yi models.

*   **Model:** `Fine_tune_Llama_3_with_ORPO.ipynb` (**Llama**)
    *   **Method:** ORPO
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [Fine_tune_Llama_3_with_ORPO.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Fine_tune_Llama_3_with_ORPO.ipynb)
    *   **Description:** Fine-tuning Llama 3 model with Odds Ratio Policy Optimization (ORPO).

*   **Model:** `Finetune_Llama3_with_LLaMA_Factory.ipynb` (**Llama**)
    *   **Method:** LLaMA-Factory
    *   **Library:** LLaMA-Factory
    *   **Dataset Type:** General
    *   **Notebook:** [Finetune_Llama3_with_LLaMA_Factory.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Finetune_Llama3_with_LLaMA_Factory.ipynb)
    *   **Description:** Fine-tuning Llama 3 model using LLaMA-Factory.

*   **Model:** `Gemma_Fine_tuning.ipynb` (**Gemma**)
    *   **Method:** General
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [Gemma_Fine_tuning.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Gemma_Fine_tuning.ipynb)
    *   **Description:** General fine-tuning notebook for Gemma models.

*   **Model:** `GPT_2_Fine_Tuning_w_Hugging_Face_&_PyTorch.ipynb` (**GPT**)
    *   **Method:** General
    *   **Library:** Hugging Face, PyTorch
    *   **Dataset Type:** General
    *   **Notebook:** [GPT_2_Fine_Tuning_w_Hugging_Face_&_PyTorch.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/GPT_2_Fine_Tuning_w_Hugging_Face_&_PyTorch.ipynb)
    *   **Description:** Fine-tuning GPT-2 model with Hugging Face Transformers and PyTorch.

*   **Model:** `LazyAxolotl_Jamba.ipynb` (**Other**)
    *   **Method:** Axolotl
    *   **Library:** Axolotl
    *   **Dataset Type:** General
    *   **Notebook:** [LazyAxolotl_Jamba.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/LazyAxolotl_Jamba.ipynb)
    *   **Description:** Fine-tuning Jamba model using LazyAxolotl.

*   **Model:** `Llama_3_2_1B+3B_Conversational_+_2x_faster_finetuning.ipynb` (**Llama**)
    *   **Method:** General, Faster Finetuning
    *   **Library:**
    *   **Dataset Type:** Conversational
    *   **Notebook:** [Llama_3_2_1B+3B_Conversational_+_2x_faster_finetuning.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/Llama_3_2_1B+3B_Conversational_+_2x_faster_finetuning.ipynb)
    *   **Description:** Conversational fine-tuning for Llama 3 2.1B and 3B models with faster fine-tuning techniques.

*   **Model:** `openai_gpt_4o_fine_tuning.ipynb` (**GPT**)
    *   **Method:** General
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [openai_gpt_4o_fine_tuning.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/openai_gpt_4o_fine_tuning.ipynb)
    *   **Description:** Fine-tuning notebook for OpenAI's GPT-4o model.

*   **Model:** `torchtune_examples.ipynb` (**Other**)
    *   **Method:** TorchTune
    *   **Library:** TorchTune
    *   **Dataset Type:** Examples
    *   **Notebook:** [torchtune_examples.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/torchtune_examples.ipynb)
    *   **Description:** Example notebooks demonstrating fine-tuning with TorchTune.

*   **Model:** `⚡Online_DPO.ipynb` (**Other**)
    *   **Method:** Online DPO
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [⚡Online_DPO.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/%E2%9A%A1Online_DPO.ipynb)
    *   **Description:** Notebook for Online Direct Preference Optimization (DPO).

*   **Model:** `[[Gemma_1]Finetune_distributed.ipynb` (**Gemma**)
    *   **Method:** Distributed Finetuning
    *   **Library:**
    *   **Dataset Type:** Chat
    *   **Notebook:** [[[Gemma_1]Finetune_distributed.ipynb]([Gemma_1]Finetune_distributed.ipynb)]([Gemma_1]Finetune_distributed.ipynb)
    *   **Description:** Distributed fine-tuning of Gemma model for chat, demonstrating response generation in a pirate's tone.

*   **Model:** `[[Gemma_1]Finetune_with_LLaMA_Factory.ipynb` (**Gemma**)
    *   **Method:** LLaMA-Factory
    *   **Library:** LLaMA-Factory
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_1]Finetune_with_LLaMA_Factory.ipynb]([Gemma_1]Finetune_with_LLaMA_Factory.ipynb)]([Gemma_1]Finetune_with_LLaMA_Factory.ipynb)
    *   **Description:** Fine-tuning Gemma model using the LLaMA-Factory library.

*   **Model:** `[[Gemma_1]Finetune_with_XTuner.ipynb` (**Gemma**)
    *   **Method:** XTuner
    *   **Library:** XTuner
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_1]Finetune_with_XTuner.ipynb]([Gemma_1]Finetune_with_XTuner.ipynb)]([Gemma_1]Finetune_with_XTuner.ipynb)
    *   **Description:** Fine-tuning Gemma model using the XTuner library.

*   **Model:** `[[Gemma_2]Custom_Vocabulary.ipynb` (**Gemma**)
    *   **Method:** Custom Vocabulary
    *   **Library:**
    *   **Dataset Type:** Tokenization
    *   **Notebook:** [[[Gemma_2]Custom_Vocabulary.ipynb]([Gemma_2]Custom_Vocabulary.ipynb)]([Gemma_2]Custom_Vocabulary.ipynb)
    *   **Description:** Demonstrates using custom vocabulary tokens "<unused[0-98]>" in Gemma models.

*   **Model:** `[[Gemma_2]Finetune_with_Axolotl.ipynb` (**Gemma**)
    *   **Method:** Axolotl
    *   **Library:** Axolotl
    *   **Dataset Type:** General
    *   **Notebook:** [[gemma_2_axolotl.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma_2_axolotl.ipynb)]([Gemma_2]Finetune_with_Axolotl.ipynb)
    *   **Description:** Fine-tuning Gemma 2 model using the Axolotl library.

*   **Model:** `[[Gemma_2]Finetune_with_Axolotl.ipynb` (**Gemma**)
    *   **Method:** Axolotl
    *   **Library:** Axolotl
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_Axolotl.ipynb]([Gemma_2]Finetune_with_Axolotl.ipynb)]([Gemma_2]Finetune_with_Axolotl.ipynb)
    *   **Description:** Fine-tuning Gemma 2 model using the Axolotl library.

*   **Model:** `[[Gemma_2]Finetune_with_CALM.ipynb` (**Gemma**)
    *   **Method:** CALM
    *   **Library:** CALM
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_CALM.ipynb]([Gemma_2]Finetune_with_CALM.ipynb)]([Gemma_2]Finetune_with_CALM.ipynb)
    *   **Description:** Fine-tuning Gemma model using the CALM library.

*   **Model:** `[[Gemma_2]Finetune_with_Function_Calling.ipynb` (**Gemma**)
    *   **Method:** Function Calling
    *   **Library:** PyTorch/XLA
    *   **Dataset Type:** Function Calling
    *   **Notebook:** [[[Gemma_2]Finetune_with_Function_Calling.ipynb]([Gemma_2]Finetune_with_Function_Calling.ipynb)]([Gemma_2]Finetune_with_Function_Calling.ipynb)
    *   **Description:** Fine-tuning Gemma for Function Calling using PyTorch/XLA.

*   **Model:** `[[Gemma_2]Finetune_with_JORA.ipynb` (**Gemma**)
    *   **Method:** JORA
    *   **Library:** JORA
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_JORA.ipynb]([Gemma_2]Finetune_with_JORA.ipynb)]([Gemma_2]Finetune_with_JORA.ipynb)
    *   **Description:** Fine-tuning Gemma model using the JORA library.

*   **Model:** `[[Gemma_2]Finetune_with_LitGPT.ipynb` (**Gemma**)
    *   **Method:** LitGPT
    *   **Library:** LitGPT
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_LitGPT.ipynb]([Gemma_2]Finetune_with_LitGPT.ipynb)]([Gemma_2]Finetune_with_LitGPT.ipynb)
    *   **Description:** Fine-tuning Gemma model using the LitGPT library.

*   **Model:** `[[Gemma_2]Finetune_with_Torch_XLA.ipynb` (**Gemma**)
    *   **Method:** General
    *   **Library:** PyTorch/XLA
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_Torch_XLA.ipynb]([Gemma_2]Finetune_with_Torch_XLA.ipynb)]([Gemma_2]Finetune_with_Torch_XLA.ipynb)
    *   **Description:** Fine-tuning Gemma model using PyTorch/XLA.

*   **Model:** `[[Gemma_2]Finetune_with_Unsloth.ipynb` (**Gemma**)
    *   **Method:** Unsloth
    *   **Library:** Unsloth
    *   **Dataset Type:** General
    *   **Notebook:** [[[Gemma_2]Finetune_with_Unsloth.ipynb]([Gemma_2]Finetune_with_Unsloth.ipynb)]([Gemma_2]Finetune_with_Unsloth.ipynb)
    *   **Description:** Fine-tuning Gemma model using the Unsloth library.

*   **Model:** `[[Gemma_2]Translator_of_Old_Korean_Literature.ipynb` (**Gemma**)
    *   **Method:** Translation
    *   **Library:** Keras
    *   **Dataset Type:** Translation
    *   **Notebook:** [[[Gemma_2]Translator_of_Old_Korean_Literature.ipynb]([Gemma_2]Translator_of_Old_Korean_Literature.ipynb)]([Gemma_2]Translator_of_Old_Korean_Literature.ipynb)
    *   **Description:** Using Gemma model to translate old Korean literature with Keras.

*   **Model:** `fine_tuning_phi_3_mini_lora.ipynb` (**Phi**)
    *   **Method:** LoRA
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [fine_tuning_phi_3_mini_lora.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/fine_tuning_phi_3_mini_lora.ipynb)
    *   **Description:** Fine-tuning Phi-3-mini model using LoRA.

*   **Model:** `fine_tuning_phi_3_mini_lora_unsloth.ipynb` (**Phi**)
    *   **Method:** LoRA
    *   **Library:** Unsloth
    *   **Dataset Type:** General
    *   **Notebook:** [fine_tuning_phi_3_mini_lora_unsloth.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/fine_tuning_phi_3_mini_lora_unsloth.ipynb)
    *   **Description:** Fine-tuning Phi-3-mini model with LoRA and Unsloth library.

*   **Model:** `finetune_paligemma_on_multiple_detection_dataset.ipynb` (**Gemma**)
    *   **Method:** General
    *   **Library:**
    *   **Dataset Type:** Detection Dataset
    *   **Notebook:** [finetune_paligemma_on_multiple_detection_dataset.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/finetune_paligemma_on_multiple_detection_dataset.ipynb)
    *   **Description:** Fine-tuning PaliGemma on multiple object detection datasets.

*   **Model:** `gemma-2_2b_qlora.ipynb` (**Gemma**)
    *   **Method:** QLoRA
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [gemma-2_2b_qlora.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma-2_2b_qlora.ipynb)
    *   **Description:** Fine-tuning Gemma 2 2B model using QLoRA.

*   **Model:** `gemma-2_9b_qlora.ipynb` (**Gemma**)
    *   **Method:** QLoRA
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [gemma_2_9b_qlora.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma_2_9b_qlora.ipynb)
    *   **Description:** Fine-tuning Gemma 2 9B model using QLoRA.

*   **Model:** `gemma_2_9b_qlora_unsloth.ipynb` (**Gemma**)
    *   **Method:** QLoRA
    *   **Library:** Unsloth
    *   **Dataset Type:** General
    *   **Notebook:** [gemma_2_9b_qlora_unsloth.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma_2_9b_qlora_unsloth.ipynb)
    *   **Description:** Fine-tuning Gemma 2 9B model with QLoRA and Unsloth library for optimization.

*   **Model:** `gemma_2_axolotl.ipynb` (**Gemma**)
    *   **Method:** Axolotl
    *   **Library:** Axolotl
    *   **Dataset Type:** General
    *   **Notebook:** [gemma_2_axolotl.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma_2_axolotl.ipynb)
    *   **Description:** Fine-tuning Gemma 2 model using the Axolotl library.

*   **Model:** `gemma2(2b)_fc_ft.ipynb` (**Gemma**)
    *   **Method:** Full Fine-tuning
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [gemma2(2b)_fc_ft.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma2(2b)_fc_ft.ipynb)
    *   **Description:** Full fine-tuning of Gemma 2 2B model.

*   **Model:** `gemma_2b_qlora.ipynb` (**Gemma**)
    *   **Method:** QLoRA
    *   **Library:**
    *   **Dataset Type:** General
    *   **Notebook:** [gemma_2b_qlora.ipynb](https://github.com/mshojaei77/Awesome-Fine-tuning/blob/main/gemma_2b_qlora.ipynb)
    *   **Description:** Fine-tuning Gemma 2B model using QLoRA.
