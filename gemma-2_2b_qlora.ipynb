{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-04T18:38:51.729008Z","iopub.status.busy":"2025-03-04T18:38:51.728657Z","iopub.status.idle":"2025-03-04T18:38:54.224233Z","shell.execute_reply":"2025-03-04T18:38:54.223379Z","shell.execute_reply.started":"2025-03-04T18:38:51.728974Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/axolotl\n"]}],"source":["!git clone -q https://github.com/OpenAccess-AI-Collective/axolotl\n","%cd axolotl"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:17:58.167310Z","iopub.status.busy":"2025-03-04T19:17:58.166996Z","iopub.status.idle":"2025-03-04T19:19:20.887287Z","shell.execute_reply":"2025-03-04T19:19:20.886408Z","shell.execute_reply.started":"2025-03-04T19:17:58.167285Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building editable for axolotl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q torch\n","!pip install -q \"axolotl[flash-attn]\" --no-build-isolation\n","!pip install -q packaging huggingface_hub\n","!pip install -q -e '.[flash-attn,deepspeed]'"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:19:41.961835Z","iopub.status.busy":"2025-03-04T19:19:41.961533Z","iopub.status.idle":"2025-03-04T19:19:41.966677Z","shell.execute_reply":"2025-03-04T19:19:41.965765Z","shell.execute_reply.started":"2025-03-04T19:19:41.961808Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["import os\n","os.environ[\"HF_TOKEN\"] = \"your token\"  "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:19:50.465337Z","iopub.status.busy":"2025-03-04T19:19:50.465029Z","iopub.status.idle":"2025-03-04T19:19:50.504822Z","shell.execute_reply":"2025-03-04T19:19:50.504147Z","shell.execute_reply.started":"2025-03-04T19:19:50.465309Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["import yaml\n","\n","yaml_string = \"\"\"\n","base_model: google/gemma-2-2b-it\n","model_type: AutoModelForCausalLM\n","tokenizer_type: AutoTokenizer\n","trust_remote_code: true\n","\n","load_in_8bit: false\n","load_in_4bit: true\n","strict: false\n","\n","chat_template: gemma\n","datasets:\n","  - path: mshojaei77/Persian_sft\n","    type: chat_template\n","    drop_system_message: true\n","    # field_messages: messages\n","\n","val_set_size: 0.0\n","output_dir: ./outputs/out\n","\n","adapter: qlora\n","lora_r: 32\n","lora_alpha: 16\n","lora_dropout: 0.05\n","lora_target_linear: true\n","\n","sequence_len: 2048\n","sample_packing: true\n","eval_sample_packing: false\n","pad_to_sequence_len: true\n","\n","wandb_project:\n","wandb_entity:\n","wandb_watch:\n","wandb_name:\n","wandb_log_model:\n","\n","\n","gradient_accumulation_steps: 1\n","micro_batch_size: 1\n","num_epochs: 4\n","max_steps: 20\n","optimizer: paged_adamw_32bit\n","lr_scheduler: cosine\n","learning_rate: 0.0002\n","\n","train_on_inputs: false\n","group_by_length: false\n","bf16: false\n","fp16: true\n","tf32: false\n","\n","gradient_checkpointing: true\n","early_stopping_patience:\n","resume_from_checkpoint:\n","local_rank:\n","logging_steps: 1\n","xformers_attention:\n","flash_attention: false\n","\n","warmup_steps: 10\n","evals_per_epoch:\n","saves_per_epoch:\n","debug:\n","deepspeed:\n","weight_decay: 0.0\n","fsdp:\n","fsdp_config:\n","special_tokens:\n","\"\"\"\n","\n","# Convert the YAML string to a Python dictionary\n","yaml_dict = yaml.safe_load(yaml_string)\n","\n","# Specify your file path\n","yaml_file = 'config.yaml'\n","\n","# Write the YAML file\n","with open(yaml_file, 'w') as file:\n","    yaml.dump(yaml_dict, file)\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:20:02.886961Z","iopub.status.busy":"2025-03-04T19:20:02.886629Z","iopub.status.idle":"2025-03-04T19:35:51.348160Z","shell.execute_reply":"2025-03-04T19:35:51.347207Z","shell.execute_reply.started":"2025-03-04T19:20:02.886932Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-04 19:20:21.622384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-04 19:20:21.622338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-04 19:20:22.160247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-04 19:20:22.160264: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-04 19:20:22.291369: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-04 19:20:22.291366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","[2025-03-04 19:20:37,902] [INFO] [datasets.<module>:54] [PID:661] PyTorch version 2.5.1+cu121 available.\n","[2025-03-04 19:20:37,903] [INFO] [datasets.<module>:54] [PID:662] PyTorch version 2.5.1+cu121 available.\n","[2025-03-04 19:20:37,903] [INFO] [datasets.<module>:66] [PID:661] Polars version 1.9.0 available.\n","[2025-03-04 19:20:37,904] [INFO] [datasets.<module>:66] [PID:662] Polars version 1.9.0 available.\n","[2025-03-04 19:20:37,904] [INFO] [datasets.<module>:77] [PID:661] Duckdb version 1.1.3 available.\n","[2025-03-04 19:20:37,905] [INFO] [datasets.<module>:77] [PID:662] Duckdb version 1.1.3 available.\n","[2025-03-04 19:20:37,905] [INFO] [datasets.<module>:112] [PID:661] TensorFlow version 2.17.1 available.\n","[2025-03-04 19:20:37,905] [INFO] [datasets.<module>:112] [PID:662] TensorFlow version 2.17.1 available.\n","[2025-03-04 19:20:37,906] [INFO] [datasets.<module>:125] [PID:661] JAX version 0.4.33 available.\n","[2025-03-04 19:20:37,906] [INFO] [datasets.<module>:125] [PID:662] JAX version 0.4.33 available.\n","[2025-03-04 19:20:40,725] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2025-03-04 19:20:40,731] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2025-03-04 19:20:40,819] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp2i5dr8ac/test.c -o /tmp/tmp2i5dr8ac/test.o\n","[2025-03-04 19:20:40,826] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmptfw4sbmc/test.c -o /tmp/tmptfw4sbmc/test.o\n","[2025-03-04 19:20:40,840] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc /tmp/tmp2i5dr8ac/test.o -laio -o /tmp/tmp2i5dr8ac/a.out\n","[2025-03-04 19:20:40,846] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc /tmp/tmptfw4sbmc/test.o -laio -o /tmp/tmptfw4sbmc/a.out\n","[2025-03-04 19:20:41,525] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpvd2vaq6o/test.c -o /tmp/tmpvd2vaq6o/test.o\n","[2025-03-04 19:20:41,535] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpawg8dt1f/test.c -o /tmp/tmpawg8dt1f/test.o\n","[2025-03-04 19:20:41,555] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc /tmp/tmpawg8dt1f/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpawg8dt1f/a.out\n","[2025-03-04 19:20:41,556] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc /tmp/tmpvd2vaq6o/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpvd2vaq6o/a.out\n","[2025-03-04 19:20:41,614] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmptr0ohlg8/test.c -o /tmp/tmptr0ohlg8/test.o\n","[2025-03-04 19:20:41,614] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmphi86e_3a/test.c -o /tmp/tmphi86e_3a/test.o\n","[2025-03-04 19:20:41,637] [INFO] [root.spawn:60] [PID:661] x86_64-linux-gnu-gcc /tmp/tmptr0ohlg8/test.o -laio -o /tmp/tmptr0ohlg8/a.out\n","[2025-03-04 19:20:41,638] [INFO] [root.spawn:60] [PID:662] x86_64-linux-gnu-gcc /tmp/tmphi86e_3a/test.o -laio -o /tmp/tmphi86e_3a/a.out\n","/kaggle/working/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n","  from torch.distributed.optim import ZeroRedundancyOptimizer\n","/kaggle/working/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n","  from torch.distributed.optim import ZeroRedundancyOptimizer\n","[2025-03-04 19:20:46,351] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1291] [PID:662] [RANK:1] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n","\u001b[33m[2025-03-04 19:20:46,352] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:1051] [PID:662] [RANK:1] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n","\u001b[33m[2025-03-04 19:20:46,352] [WARNING] [axolotl.utils.config.models.input.hint_trust_remote_code:460] [PID:662] [RANK:1] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n","[2025-03-04 19:20:46,352] [INFO] [axolotl.utils.config.models.input.check_bf16:1695] [PID:662] [RANK:1] bf16 support detected, but not enabled for this configuration.\u001b[39m\n","[2025-03-04 19:20:46,352] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1291] [PID:661] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n","\u001b[33m[2025-03-04 19:20:46,352] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:1051] [PID:661] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n","\u001b[33m[2025-03-04 19:20:46,353] [WARNING] [axolotl.utils.config.models.input.hint_trust_remote_code:460] [PID:661] [RANK:0] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n","[2025-03-04 19:20:46,353] [INFO] [axolotl.utils.config.models.input.check_bf16:1695] [PID:661] [RANK:0] bf16 support detected, but not enabled for this configuration.\u001b[39m\n","config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 6.14MB/s]\n","[2025-03-04 19:20:46,744] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:661] [RANK:0] cuda memory usage baseline: 0.000GB (+0.002GB cache, +0.459GB misc)\u001b[39m\n","\n","     #@@ #@@      @@# @@#\n","    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n","    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n","      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n","    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n","    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n","     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n","                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n","    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n","                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n","    @@@@  @@@@@@@@@@@@@@@@\n","\n","[2025-03-04 19:20:46,788] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:662] [RANK:1] cuda memory usage baseline: 0.000GB (+0.262GB misc)\u001b[39m\n","tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 2.20MB/s]\n","tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 53.8MB/s]\n","tokenizer.json: 100%|███████████████████████| 17.5M/17.5M [00:00<00:00, 107MB/s]\n","special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 4.98MB/s]\n","[2025-03-04 19:20:49,445] [DEBUG] [axolotl.load_tokenizer:298] [PID:661] [RANK:0] EOS: 1 / <eos>\u001b[39m\n","[2025-03-04 19:20:49,445] [DEBUG] [axolotl.load_tokenizer:299] [PID:661] [RANK:0] BOS: 2 / <bos>\u001b[39m\n","[2025-03-04 19:20:49,445] [DEBUG] [axolotl.load_tokenizer:300] [PID:661] [RANK:0] PAD: 0 / <pad>\u001b[39m\n","[2025-03-04 19:20:49,446] [DEBUG] [axolotl.load_tokenizer:301] [PID:661] [RANK:0] UNK: 3 / <unk>\u001b[39m\n","[2025-03-04 19:20:49,446] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:661] [RANK:0] Unable to find prepared dataset in last_run_prepared/cbfc1d855cb71f4d172991defb6a48b6\u001b[39m\n","[2025-03-04 19:20:49,446] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:661] [RANK:0] Loading raw datasets...\u001b[39m\n","\u001b[33m[2025-03-04 19:20:49,446] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:661] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n","[2025-03-04 19:20:49,446] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:661] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n","[2025-03-04 19:20:49,474] [DEBUG] [axolotl.load_tokenizer:298] [PID:662] [RANK:1] EOS: 1 / <eos>\u001b[39m\n","[2025-03-04 19:20:49,475] [DEBUG] [axolotl.load_tokenizer:299] [PID:662] [RANK:1] BOS: 2 / <bos>\u001b[39m\n","[2025-03-04 19:20:49,475] [DEBUG] [axolotl.load_tokenizer:300] [PID:662] [RANK:1] PAD: 0 / <pad>\u001b[39m\n","[2025-03-04 19:20:49,475] [DEBUG] [axolotl.load_tokenizer:301] [PID:662] [RANK:1] UNK: 3 / <unk>\u001b[39m\n","[rank1]:[W304 19:20:49.390446900 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n","README.md: 100%|███████████████████████████████| 371/371 [00:00<00:00, 2.21MB/s]\n","train-00000-of-00002.parquet: 100%|█████████▉| 103M/103M [00:01<00:00, 91.8MB/s]\n","train-00001-of-00002.parquet: 100%|██████████▉| 123M/123M [00:00<00:00, 165MB/s]\n","Generating train split: 100%|█| 681168/681168 [00:01<00:00, 388443.40 examples/s\n","[2025-03-04 19:20:56,688] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:661] [RANK:0] Loading dataset with base_type: chat_template and prompt_style: None\u001b[39m\n","[2025-03-04 19:20:56,757] [INFO] [axolotl.__call__:573] [PID:661] [RANK:0] Using chat template:\n","---\n","{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n","' + message['content'] | trim + '<end_of_turn>\n","' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n","'}}{% endif %}\n","---\u001b[39m\n","Tokenizing Prompts (num_proc=4): 100%|█| 681168/681168 [10:00<00:00, 1134.05 exa\n","[2025-03-04 19:30:58,816] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:661] [RANK:0] min_input_len: 20\u001b[39m\n","[2025-03-04 19:30:58,818] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:661] [RANK:0] max_input_len: 3348\u001b[39m\n","Dropping Long Sequences (num_proc=4): 100%|█| 681168/681168 [00:21<00:00, 31552.\n","\u001b[33m[2025-03-04 19:31:22,622] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:207] [PID:661] [RANK:0] Dropped 19 long samples from dataset\u001b[39m\n","Drop Samples with Zero Trainable Tokens (num_proc=4): 100%|█| 681149/681149 [00:\n","Add position_id column (Sample Packing) (num_proc=4): 100%|█| 681149/681149 [00:\n","[2025-03-04 19:32:44,450] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:332] [PID:661] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/cbfc1d855cb71f4d172991defb6a48b6\u001b[39m\n","Saving the dataset (4/4 shards): 100%|█| 681149/681149 [00:02<00:00, 315678.57 e\n","[rank0]:[W304 19:32:46.584845864 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n","[2025-03-04 19:32:47,414] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:662] [RANK:1] Unable to find prepared dataset in last_run_prepared/cbfc1d855cb71f4d172991defb6a48b6\u001b[39m\n","[2025-03-04 19:32:47,414] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:662] [RANK:1] Loading raw datasets...\u001b[39m\n","\u001b[33m[2025-03-04 19:32:47,414] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:662] [RANK:1] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n","[2025-03-04 19:32:47,414] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:662] [RANK:1] No seed provided, using default seed of 42\u001b[39m\n","[2025-03-04 19:32:49,758] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:662] [RANK:1] Loading dataset with base_type: chat_template and prompt_style: None\u001b[39m\n","[2025-03-04 19:32:49,825] [INFO] [axolotl.__call__:573] [PID:662] [RANK:1] Using chat template:\n","---\n","{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n","' + message['content'] | trim + '<end_of_turn>\n","' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n","'}}{% endif %}\n","---\u001b[39m\n","[2025-03-04 19:32:51,155] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:662] [RANK:1] min_input_len: 20\u001b[39m\n","[2025-03-04 19:32:51,157] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:662] [RANK:1] max_input_len: 3348\u001b[39m\n","\u001b[33m[2025-03-04 19:32:53,142] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:207] [PID:662] [RANK:1] Dropped 19 long samples from dataset\u001b[39m\n","[2025-03-04 19:33:23,946] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:203] [PID:661] [RANK:0] gather_len_batches: [48434, 48431]\u001b[39m\n","[2025-03-04 19:33:23,975] [INFO] [axolotl.log:63] [PID:661] [RANK:0] sample_packing_eff_est across ranks: [0.9507461190223694, 0.9507865905761719]\u001b[39m\n","[2025-03-04 19:33:23,977] [INFO] [axolotl.utils.data.sft.prepare_dataset:166] [PID:662] [RANK:1] Maximum number of steps set at 20\u001b[39m\n","[2025-03-04 19:33:23,977] [INFO] [axolotl.utils.data.sft.prepare_dataset:166] [PID:661] [RANK:0] Maximum number of steps set at 20\u001b[39m\n","model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 19.8MB/s]\n","Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00002.safetensors:   0%|    | 10.5M/4.99G [00:01<09:02, 9.18MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   0%|    | 21.0M/4.99G [00:01<05:19, 15.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1%|    | 31.5M/4.99G [00:02<05:42, 14.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1%|    | 62.9M/4.99G [00:02<02:08, 38.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3%|▏    | 147M/4.99G [00:02<00:53, 91.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3%|▏    | 168M/4.99G [00:03<01:01, 78.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4%|▏    | 189M/4.99G [00:04<01:44, 45.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4%|▏    | 199M/4.99G [00:04<01:44, 45.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4%|▏    | 210M/4.99G [00:04<01:42, 46.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5%|▏    | 241M/4.99G [00:04<01:05, 72.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5%|▎    | 262M/4.99G [00:06<02:26, 32.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6%|▎    | 283M/4.99G [00:06<01:52, 41.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6%|▎    | 304M/4.99G [00:06<01:31, 51.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8%|▍     | 398M/4.99G [00:06<00:35, 128MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9%|▌     | 440M/4.99G [00:07<00:29, 156MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12%|▋     | 577M/4.99G [00:07<00:14, 311MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13%|▊     | 640M/4.99G [00:07<00:14, 296MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14%|▊     | 692M/4.99G [00:07<00:18, 237MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15%|▉     | 734M/4.99G [00:07<00:19, 223MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15%|▉     | 765M/4.99G [00:08<00:19, 216MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17%|█     | 839M/4.99G [00:08<00:14, 287MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18%|█     | 881M/4.99G [00:08<00:14, 286MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18%|█     | 923M/4.99G [00:08<00:14, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19%|█▏    | 954M/4.99G [00:08<00:14, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21%|█    | 1.03G/4.99G [00:08<00:10, 363MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22%|█    | 1.08G/4.99G [00:08<00:10, 386MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23%|█▏   | 1.13G/4.99G [00:09<00:10, 353MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24%|█▏   | 1.21G/4.99G [00:09<00:08, 438MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26%|█▎   | 1.30G/4.99G [00:09<00:07, 487MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27%|█▎   | 1.35G/4.99G [00:09<00:10, 361MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29%|█▍   | 1.44G/4.99G [00:09<00:08, 422MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30%|█▍   | 1.49G/4.99G [00:09<00:08, 436MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31%|█▌   | 1.55G/4.99G [00:09<00:07, 462MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33%|█▋   | 1.64G/4.99G [00:10<00:06, 524MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34%|█▋   | 1.71G/4.99G [00:10<00:06, 526MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36%|█▊   | 1.80G/4.99G [00:10<00:05, 611MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38%|█▉   | 1.88G/4.99G [00:10<00:04, 632MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39%|█▉   | 1.95G/4.99G [00:10<00:05, 606MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40%|██   | 2.01G/4.99G [00:10<00:05, 584MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42%|██   | 2.08G/4.99G [00:10<00:05, 579MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43%|██▏  | 2.15G/4.99G [00:10<00:04, 615MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45%|██▏  | 2.23G/4.99G [00:11<00:04, 663MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47%|██▎  | 2.35G/4.99G [00:11<00:03, 771MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49%|██▍  | 2.44G/4.99G [00:11<00:03, 767MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51%|██▌  | 2.54G/4.99G [00:11<00:03, 785MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53%|██▋  | 2.63G/4.99G [00:11<00:02, 826MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54%|██▋  | 2.72G/4.99G [00:11<00:03, 738MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56%|██▊  | 2.80G/4.99G [00:11<00:03, 620MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58%|██▉  | 2.87G/4.99G [00:11<00:03, 579MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59%|██▉  | 2.96G/4.99G [00:12<00:03, 600MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61%|███  | 3.02G/4.99G [00:12<00:03, 538MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62%|███  | 3.10G/4.99G [00:12<00:03, 588MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63%|███▏ | 3.17G/4.99G [00:12<00:03, 557MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65%|███▎ | 3.26G/4.99G [00:12<00:02, 610MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67%|███▎ | 3.32G/4.99G [00:12<00:02, 569MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68%|███▍ | 3.41G/4.99G [00:12<00:02, 623MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71%|███▌ | 3.52G/4.99G [00:12<00:01, 751MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74%|███▋ | 3.67G/4.99G [00:13<00:01, 936MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76%|███▊ | 3.77G/4.99G [00:13<00:02, 567MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77%|███▊ | 3.86G/4.99G [00:14<00:03, 324MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79%|███▉ | 3.92G/4.99G [00:14<00:04, 240MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80%|███▏| 3.97G/4.99G [00:16<00:11, 88.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81%|███▏| 4.02G/4.99G [00:17<00:11, 82.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81%|███▏| 4.05G/4.99G [00:17<00:10, 85.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82%|███▎| 4.08G/4.99G [00:17<00:09, 93.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82%|████ | 4.10G/4.99G [00:17<00:08, 100MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84%|████▏| 4.17G/4.99G [00:17<00:05, 159MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85%|████▏| 4.22G/4.99G [00:18<00:04, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86%|████▎| 4.30G/4.99G [00:18<00:02, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89%|████▍| 4.41G/4.99G [00:18<00:01, 394MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91%|████▌| 4.56G/4.99G [00:18<00:00, 589MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96%|████▊| 4.79G/4.99G [00:18<00:00, 928MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100%|████▉| 4.99G/4.99G [00:18<00:00, 264MB/s]\u001b[A\n","Downloading shards:  50%|████████████▌            | 1/2 [00:19<00:19, 19.11s/it]\n","model-00002-of-00002.safetensors:   0%|              | 0.00/241M [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00002.safetensors:   4%|▏    | 10.5M/241M [00:00<00:07, 30.2MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  26%|█▌    | 62.9M/241M [00:00<00:01, 162MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  39%|██▎   | 93.9M/241M [00:00<00:00, 187MB/s]\u001b[A\n","model-00002-of-00002.safetensors: 100%|██████▉| 241M/241M [00:00<00:00, 312MB/s]\u001b[A\n","Downloading shards: 100%|█████████████████████████| 2/2 [00:20<00:00, 10.01s/it]\n","Downloading shards: 100%|█████████████████████████| 2/2 [00:20<00:00, 10.03s/it]\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.96s/it]\n","generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.00MB/s]\n","[2025-03-04 19:33:56,363] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:662] [RANK:1] cuda memory usage after model load: 2.121GB (+0.037GB cache, +0.459GB misc)\u001b[39m\n","[2025-03-04 19:33:56,369] [INFO] [axolotl.prepare_model:994] [PID:662] [RANK:1] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n","[2025-03-04 19:33:56,372] [INFO] [axolotl.load_model:1121] [PID:662] [RANK:1] Converting modules to torch.float16\u001b[39m\n","[2025-03-04 19:33:56,378] [INFO] [axolotl.load_lora:1311] [PID:662] [RANK:1] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.40s/it]\n","[2025-03-04 19:33:57,085] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:662] [RANK:1] cuda memory usage after adapters: 2.287GB (+2.149GB cache, +0.459GB misc)\u001b[39m\n","[2025-03-04 19:33:57,113] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:661] [RANK:0] cuda memory usage after model load: 2.121GB (+0.037GB cache, +0.557GB misc)\u001b[39m\n","[2025-03-04 19:33:57,120] [INFO] [axolotl.prepare_model:994] [PID:661] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n","[2025-03-04 19:33:57,124] [INFO] [axolotl.load_model:1121] [PID:661] [RANK:0] Converting modules to torch.float16\u001b[39m\n","[2025-03-04 19:33:57,129] [INFO] [axolotl.load_lora:1311] [PID:661] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n","trainable params: 41,533,440 || all params: 2,655,875,328 || trainable%: 1.5638\n","[2025-03-04 19:33:57,833] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:661] [RANK:0] cuda memory usage after adapters: 2.287GB (+2.149GB cache, +0.557GB misc)\u001b[39m\n","/kaggle/working/axolotl/src/axolotl/core/trainers/base.py:177: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(*_args, **kwargs)\n","No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","/kaggle/working/axolotl/src/axolotl/core/trainers/base.py:177: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(*_args, **kwargs)\n","No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","[2025-03-04 19:33:59,903] [INFO] [axolotl.train.log:63] [PID:661] [RANK:0] Pre-saving adapter config to ./outputs/out\u001b[39m\n","[2025-03-04 19:34:00,470] [INFO] [axolotl.train.log:63] [PID:661] [RANK:0] Starting trainer...\u001b[39m\n","[2025-03-04 19:34:15,780] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:203] [PID:661] [RANK:0] gather_len_batches: [48422, 48422]\u001b[39m\n","  0%|                                                    | 0/20 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n","It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n","{'loss': 5.3498, 'grad_norm': 1.9392071962356567, 'learning_rate': 2e-05, 'epoch': 0.0}\n","  5%|██▏                                         | 1/20 [00:06<02:04,  6.56s/it][2025-03-04 19:34:27,224] [INFO] [axolotl.callbacks.log_gpu_memory_usage:106] [PID:662] [RANK:1] cuda memory usage while training: 2.597GB (+8.886GB cache, +0.855GB misc)\u001b[39m\n","[2025-03-04 19:34:27,231] [INFO] [axolotl.callbacks.log_gpu_memory_usage:106] [PID:661] [RANK:0] cuda memory usage while training: 2.597GB (+8.886GB cache, +0.953GB misc)\u001b[39m\n","{'loss': 5.7631, 'grad_norm': 2.240938901901245, 'learning_rate': 4e-05, 'epoch': 0.0}\n","{'loss': 4.7104, 'grad_norm': 1.5801031589508057, 'learning_rate': 6e-05, 'epoch': 0.0}\n","{'loss': 5.1583, 'grad_norm': 1.2472320795059204, 'learning_rate': 8e-05, 'epoch': 0.0}\n","{'loss': 5.2155, 'grad_norm': 1.284318208694458, 'learning_rate': 0.0001, 'epoch': 0.0}\n","{'loss': 4.6802, 'grad_norm': 1.1534394025802612, 'learning_rate': 0.00012, 'epoch': 0.0}\n","{'loss': 4.9235, 'grad_norm': 0.8322233557701111, 'learning_rate': 0.00014, 'epoch': 0.0}\n","{'loss': 4.8562, 'grad_norm': 1.1747784614562988, 'learning_rate': 0.00016, 'epoch': 0.0}\n","{'loss': 4.7394, 'grad_norm': 1.4636634588241577, 'learning_rate': 0.00018, 'epoch': 0.0}\n","{'loss': 4.5677, 'grad_norm': 1.1292502880096436, 'learning_rate': 0.0002, 'epoch': 0.0}\n","{'loss': 4.7323, 'grad_norm': 1.3883109092712402, 'learning_rate': 0.00019510565162951537, 'epoch': 0.0}\n","{'loss': 4.7172, 'grad_norm': 1.2090983390808105, 'learning_rate': 0.00018090169943749476, 'epoch': 0.0}\n","{'loss': 4.3666, 'grad_norm': 0.9335730671882629, 'learning_rate': 0.00015877852522924732, 'epoch': 0.0}\n","{'loss': 4.1881, 'grad_norm': 1.4107939004898071, 'learning_rate': 0.00013090169943749476, 'epoch': 0.0}\n","{'loss': 4.0277, 'grad_norm': 1.347548246383667, 'learning_rate': 0.0001, 'epoch': 0.0}\n","{'loss': 4.2416, 'grad_norm': 1.2607060670852661, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.0}\n","{'loss': 3.9804, 'grad_norm': 1.011076807975769, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.0}\n","{'loss': 4.2743, 'grad_norm': 0.984136164188385, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.0}\n","{'loss': 4.278, 'grad_norm': 1.0869176387786865, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.0}\n","{'loss': 4.1756, 'grad_norm': 0.9164448380470276, 'learning_rate': 0.0, 'epoch': 0.0}\n","{'train_runtime': 88.6313, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.226, 'train_loss': 4.64728991985321, 'epoch': 0.0}\n","100%|███████████████████████████████████████████| 20/20 [01:28<00:00,  4.43s/it]\n","[2025-03-04 19:35:44,715] [INFO] [axolotl.train.log:63] [PID:661] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/out\u001b[39m\n","\u001b[0m\u001b[0m[rank0]:[W304 19:35:47.106762647 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","Done\n"]}],"source":["!accelerate launch -m axolotl.cli.train config.yaml\n","print(\"Done\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:37:14.927443Z","iopub.status.busy":"2025-03-04T19:37:14.927068Z","iopub.status.idle":"2025-03-04T19:38:01.265216Z","shell.execute_reply":"2025-03-04T19:38:01.263989Z","shell.execute_reply.started":"2025-03-04T19:37:14.927413Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-04 19:37:21.739223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-04 19:37:21.762585: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-04 19:37:21.769487: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","[2025-03-04 19:37:28,296] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","/kaggle/working/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n","  from torch.distributed.optim import ZeroRedundancyOptimizer\n","[2025-03-04 19:37:30,932] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1291] [PID:1140] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n","\u001b[33m[2025-03-04 19:37:30,933] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:1051] [PID:1140] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n","\u001b[33m[2025-03-04 19:37:30,933] [WARNING] [axolotl.utils.config.models.input.hint_trust_remote_code:460] [PID:1140] [RANK:0] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n","[2025-03-04 19:37:30,933] [INFO] [axolotl.utils.config.models.input.check_bf16:1695] [PID:1140] [RANK:0] bf16 support detected, but not enabled for this configuration.\u001b[39m\n","[2025-03-04 19:37:31,148] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:1140] [RANK:0] cuda memory usage baseline: 0.000GB (+0.002GB cache, +0.359GB misc)\u001b[39m\n","\n","     #@@ #@@      @@# @@#\n","    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n","    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n","      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n","    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n","    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n","     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n","                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n","    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n","                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n","    @@@@  @@@@@@@@@@@@@@@@\n","\n","[2025-03-04 19:37:31,178] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:311] [PID:1140] [RANK:0] loading tokenizer... google/gemma-2-2b-it\u001b[39m\n","[2025-03-04 19:37:32,454] [DEBUG] [axolotl.load_tokenizer:298] [PID:1140] [RANK:0] EOS: 1 / <eos>\u001b[39m\n","[2025-03-04 19:37:32,454] [DEBUG] [axolotl.load_tokenizer:299] [PID:1140] [RANK:0] BOS: 2 / <bos>\u001b[39m\n","[2025-03-04 19:37:32,454] [DEBUG] [axolotl.load_tokenizer:300] [PID:1140] [RANK:0] PAD: 0 / <pad>\u001b[39m\n","[2025-03-04 19:37:32,454] [DEBUG] [axolotl.load_tokenizer:301] [PID:1140] [RANK:0] UNK: 3 / <unk>\u001b[39m\n","[2025-03-04 19:37:32,454] [INFO] [axolotl.cli.utils.load_model_and_tokenizer:314] [PID:1140] [RANK:0] loading model...\u001b[39m\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.00s/it]\n","[2025-03-04 19:37:39,068] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:1140] [RANK:0] cuda memory usage after model load: 2.985GB (+0.107GB cache, +0.359GB misc)\u001b[39m\n","[2025-03-04 19:37:39,086] [INFO] [axolotl.load_model:1121] [PID:1140] [RANK:0] Converting modules to torch.float16\u001b[39m\n","[2025-03-04 19:37:39,092] [INFO] [axolotl.load_lora:1311] [PID:1140] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n","[2025-03-04 19:37:39,092] [DEBUG] [axolotl.load_lora:1362] [PID:1140] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n","trainable params: 41,533,440 || all params: 2,655,875,328 || trainable%: 1.5638\n","[2025-03-04 19:37:40,867] [INFO] [axolotl.log_gpu_memory_usage:106] [PID:1140] [RANK:0] cuda memory usage after adapters: 3.077GB (+2.318GB cache, +0.375GB misc)\u001b[39m\n","[2025-03-04 19:37:42,133] [INFO] [__main__.do_merge_lora:33] [PID:1140] Running merge of LoRA with base model...\n","Unloading and merging model: 100%|██████████| 579/579 [00:00<00:00, 3748.46it/s]\n","[2025-03-04 19:37:42,296] [INFO] [__main__.do_merge_lora:39] [PID:1140] Saving merged model to: outputs/out/merged...\n","\u001b[0m"]}],"source":["!python3 -m axolotl.cli.merge_lora config.yaml --lora_model_dir=\"./outputs/out\""]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-03-04T19:38:54.767703Z","iopub.status.busy":"2025-03-04T19:38:54.767405Z","iopub.status.idle":"2025-03-04T19:40:57.149850Z","shell.execute_reply":"2025-03-04T19:40:57.148944Z","shell.execute_reply.started":"2025-03-04T19:38:54.767679Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"013196cdb1e04833b226c1ebcc24956c","version_major":2,"version_minor":0},"text/plain":["Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d1adebe5c3f4243ba27990a5e5abcac","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39503f1143da47c38593527174a500a3","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"797b899ae325429eb7c234ca882f8bbc","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d81974a6f63143219486f1207f3225df","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/mshojaei77/Gohar-2b/commit/ebb7d8485ff18a3b8bb0f5f0d60b340be9441468', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ebb7d8485ff18a3b8bb0f5f0d60b340be9441468', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mshojaei77/Gohar-2b', endpoint='https://huggingface.co', repo_type='model', repo_id='mshojaei77/Gohar-2b'), pr_revision=None, pr_num=None)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi\n","from google.colab import userdata\n","\n","new_model = \"mshojaei77/Gohar-2b\"\n","\n","# HF_TOKEN defined in the secrets tab in Google Colab\n","api = HfApi()\n","\n","# Upload merge folder\n","api.create_repo(\n","    repo_id=new_model,\n","    repo_type=\"model\",\n","    exist_ok=True,\n",")\n","api.upload_folder(\n","    repo_id=new_model,\n","    folder_path=\"outputs/out/merged\",\n",")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
