{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b6e3d2c38a446d59002f128f3ca0ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad48dcd3b3c3481b9060d653bc0774a9",
              "IPY_MODEL_3289a936b30646fcac5ccd3a5e73e7c0",
              "IPY_MODEL_93f307da86544651a1c3bebe5f2cfc3f",
              "IPY_MODEL_091c6f3362824d81be0c99dfa6570bc3"
            ],
            "layout": "IPY_MODEL_67adb6d3312c417b8a17c6dc2fdbc4e0"
          }
        },
        "53cddc78da1a4a298a3813d9ce83fbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aef5af4553c48938b42466358587721",
            "placeholder": "​",
            "style": "IPY_MODEL_0ec0215c203346c58717fb1ac8c86afd",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "52363b25e32c4c3aafb3221bc58ae761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db170ab0f30b4d1e87e810786c44d562",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc9e42c2e784c089050e1cd985ff0fe",
            "value": ""
          }
        },
        "2877f90bf21040b8a282847c326bdebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_82718ea9c2e04e0aaf7e9d2d87c0e9d0",
            "style": "IPY_MODEL_e59ce7e04513420e9b9fa06460a7b877",
            "value": true
          }
        },
        "dfbbb6f5b9c84dbeb6fc6e63856b861f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d4e0ac7492ab4047a1b8ac57c3907f80",
            "style": "IPY_MODEL_e2d4d1c7f53242f8bd444e7ea86095ee",
            "tooltip": ""
          }
        },
        "d382e29f336d4fc38d3fc5bfa94b50aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9fc23c6f5684d30884cf7a504639402",
            "placeholder": "​",
            "style": "IPY_MODEL_f56f1d8e285c401a91fc3c768f7a2a6a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "67adb6d3312c417b8a17c6dc2fdbc4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2aef5af4553c48938b42466358587721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec0215c203346c58717fb1ac8c86afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db170ab0f30b4d1e87e810786c44d562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc9e42c2e784c089050e1cd985ff0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82718ea9c2e04e0aaf7e9d2d87c0e9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59ce7e04513420e9b9fa06460a7b877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e0ac7492ab4047a1b8ac57c3907f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d4d1c7f53242f8bd444e7ea86095ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d9fc23c6f5684d30884cf7a504639402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56f1d8e285c401a91fc3c768f7a2a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bac30ec377e4430b30e2c72f89aa9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a0b562be51a4d02b442cf9062e47ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_f43750cfaa7b46dd9f8a31bdfe3c5308",
            "value": "Connecting..."
          }
        },
        "8a0b562be51a4d02b442cf9062e47ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43750cfaa7b46dd9f8a31bdfe3c5308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad48dcd3b3c3481b9060d653bc0774a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1159112e7c9642f98ae5d78f69e2ac41",
            "placeholder": "​",
            "style": "IPY_MODEL_fec29a249b8347fc8da8ac9faf83b18b",
            "value": "Token is valid (permission: write)."
          }
        },
        "3289a936b30646fcac5ccd3a5e73e7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_412c420876524ad6916e7406d60d9e7b",
            "placeholder": "​",
            "style": "IPY_MODEL_5b8f64329f454fa8b23c54ab9a100a8b",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "93f307da86544651a1c3bebe5f2cfc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d39af16d1674cb3a1a6c2395c2f0e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_42c3db18f08d416ca9ecc75899c34f0a",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "091c6f3362824d81be0c99dfa6570bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8750d30d5924e0488bdbdf4e8bdcdf9",
            "placeholder": "​",
            "style": "IPY_MODEL_af7d3afbe2724396b68808c62f93045e",
            "value": "Login successful"
          }
        },
        "1159112e7c9642f98ae5d78f69e2ac41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec29a249b8347fc8da8ac9faf83b18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "412c420876524ad6916e7406d60d9e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8f64329f454fa8b23c54ab9a100a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d39af16d1674cb3a1a6c2395c2f0e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c3db18f08d416ca9ecc75899c34f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8750d30d5924e0488bdbdf4e8bdcdf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af7d3afbe2724396b68808c62f93045e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/Awesome-Fine-tuning/blob/main/Finetune_Llama3_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "giM74oK1rRIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7908570b-a5a9-4f61-b50f-f7b824f3a1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15581, done.\u001b[K\n",
            "remote: Counting objects: 100% (531/531), done.\u001b[K\n",
            "remote: Compressing objects: 100% (284/284), done.\u001b[K\n",
            "remote: Total 15581 (delta 343), reused 390 (delta 246), pack-reused 15050\u001b[K\n",
            "Receiving objects: 100% (15581/15581), 221.79 MiB | 14.77 MiB/s, done.\n",
            "Resolving deltas: 100% (11392/11392), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (4.41.2)\n",
            "Collecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.8.3.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.8.3.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.8.3.dev0)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.8.2)\n",
            "Collecting fastapi (from llamafactory==0.8.3.dev0)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from llamafactory==0.8.3.dev0)\n",
            "  Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.8.3.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.25.2)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading ruff-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.1 aiofiles-23.2.1 altair-5.3.0 bitsandbytes-0.43.1 datasets-2.20.0 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.38.1 gradio-client-1.1.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.8.3.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.6 peft-0.11.1 pyarrow-16.1.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.32.3 ruff-0.5.2 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.5 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "aUXC-CBncFZt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to the JSON file\n",
        "json_file_path = \"data/dataset_info.json\"\n",
        "\n",
        "# Load the existing JSON data\n",
        "with open(json_file_path, \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Add the new dataset information\n",
        "data[\"Dolly_Alpaca_Lmsys\"] = {\n",
        "    \"hf_hub_url\": \"mshojaei77/Dolly_Alpaca_Lmsys\"\n",
        "}\n",
        "\n",
        "# Save the updated JSON data back to the file\n",
        "with open(json_file_path, \"w\") as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"Dataset information added successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH3UsWuIfO-E",
        "outputId": "461e1e3b-dced-4db8-9274-156608d9900e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset information added successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"Dolly_Alpaca_Lmsys\",             # use alpaca datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8f2807-e987-4a48-bcd6-2a444b8f5ed4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-15 13:06:50.980667: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-15 13:06:50.980723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-15 13:06:50.982050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-15 13:06:50.989415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 13:06:52.281988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/15/2024 13:07:01 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/15/2024 13:07:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 13:07:01,271 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 13:07:01,271 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 13:07:01,271 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 13:07:01,271 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-15 13:07:01,971 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/15/2024 13:07:01 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/15/2024 13:07:01 - INFO - llamafactory.data.loader - Loading dataset mshojaei77/Dolly_Alpaca_Lmsys...\n",
            "Converting format of dataset: 100% 500/500 [00:00<00:00, 8052.62 examples/s]\n",
            "Running tokenizer on dataset: 100% 500/500 [00:00<00:00, 790.19 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 4599, 1550, 11463, 8494, 1212, 10565, 5380, 64797, 8494, 11, 279, 11380, 836, 315, 11463, 8494, 35230, 80092, 12604, 11, 374, 459, 13673, 6108, 33575, 13, 1102, 374, 279, 7928, 33575, 555, 26155, 1404, 311, 1005, 279, 11463, 6883, 13, 1102, 65362, 3600, 389, 220, 2148, 6287, 220, 1049, 15, 439, 11463, 8868, 11, 449, 1403, 14467, 389, 264, 3254, 6149, 13, 1102, 15187, 1766, 5196, 439, 264, 3682, 33575, 304, 8494, 596, 13018, 3157, 1306, 279, 18678, 315, 1556, 67614, 8494, 304, 6250, 220, 1049, 16, 13, 578, 33575, 706, 2533, 15042, 311, 6089, 8854, 220, 843, 9919, 304, 8494, 11, 505, 69776, 304, 47335, 11, 27535, 323, 21972, 13, 128009, 128006, 78191, 128007, 271, 64797, 8494, 65362, 3600, 389, 220, 2148, 6287, 220, 1049, 15, 439, 11463, 8868, 11, 449, 1403, 14467, 389, 264, 3254, 6149, 13, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "When did Virgin Australia start operating?\n",
            "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 64797, 8494, 65362, 3600, 389, 220, 2148, 6287, 220, 1049, 15, 439, 11463, 8868, 11, 449, 1403, 14467, 389, 264, 3254, 6149, 13, 128009]\n",
            "labels:\n",
            "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.<|eot_id|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.15k/1.15k [00:00<00:00, 6.84MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-15 13:07:06,699 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-15 13:07:06,700 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/15/2024 13:07:06 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
            "07/15/2024 13:07:06 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:393] 2024-07-15 13:07:07,058 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "model.safetensors: 100% 5.70G/5.70G [02:11<00:00, 43.4MB/s]\n",
            "[INFO|modeling_utils.py:3474] 2024-07-15 13:09:18,761 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
            "[INFO|modeling_utils.py:1519] 2024-07-15 13:09:19,120 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-15 13:09:19,125 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4280] 2024-07-15 13:09:44,822 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-15 13:09:44,822 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 131/131 [00:00<00:00, 713kB/s]\n",
            "[INFO|configuration_utils.py:917] 2024-07-15 13:09:45,291 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-15 13:09:45,291 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "07/15/2024 13:09:45 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/15/2024 13:09:45 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/15/2024 13:09:45 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/15/2024 13:09:45 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/15/2024 13:09:45 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,v_proj,down_proj,up_proj,o_proj,q_proj,gate_proj\n",
            "07/15/2024 13:09:46 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:641] 2024-07-15 13:09:46,383 >> Using auto half precision backend\n",
            "07/15/2024 13:09:48 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2078] 2024-07-15 13:09:48,519 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-15 13:09:48,519 >>   Num examples = 500\n",
            "[INFO|trainer.py:2080] 2024-07-15 13:09:48,519 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-15 13:09:48,519 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2084] 2024-07-15 13:09:48,519 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2085] 2024-07-15 13:09:48,519 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2086] 2024-07-15 13:09:48,519 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2087] 2024-07-15 13:09:48,530 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 2.194, 'grad_norm': 2.4814419746398926, 'learning_rate': 2.368421052631579e-05, 'epoch': 0.16}\n",
            "{'loss': 1.6618, 'grad_norm': 0.7525412440299988, 'learning_rate': 5e-05, 'epoch': 0.32}\n",
            "{'loss': 1.4047, 'grad_norm': 1.6923415660858154, 'learning_rate': 4.955894203235284e-05, 'epoch': 0.48}\n",
            "{'loss': 1.7469, 'grad_norm': 0.967944324016571, 'learning_rate': 4.825133069987737e-05, 'epoch': 0.64}\n",
            "{'loss': 1.6007, 'grad_norm': 0.994124174118042, 'learning_rate': 4.637110229151863e-05, 'epoch': 0.8}\n",
            "{'loss': 1.5743, 'grad_norm': 1.231868863105774, 'learning_rate': 4.3568127285387925e-05, 'epoch': 0.96}\n",
            "{'loss': 1.2131, 'grad_norm': 1.3733245134353638, 'learning_rate': 4.0109982640576674e-05, 'epoch': 1.12}\n",
            "{'loss': 1.2389, 'grad_norm': 1.1093753576278687, 'learning_rate': 3.611868773699449e-05, 'epoch': 1.28}\n",
            "{'loss': 1.1442, 'grad_norm': 1.5777373313903809, 'learning_rate': 3.173507396811774e-05, 'epoch': 1.44}\n",
            "{'loss': 1.1012, 'grad_norm': 1.1162997484207153, 'learning_rate': 2.7113815556334478e-05, 'epoch': 1.6}\n",
            "{'loss': 1.3259, 'grad_norm': 1.0175338983535767, 'learning_rate': 2.241797192909059e-05, 'epoch': 1.76}\n",
            "{'loss': 1.1829, 'grad_norm': 1.2050042152404785, 'learning_rate': 1.7813234226115764e-05, 'epoch': 1.92}\n",
            "{'loss': 0.974, 'grad_norm': 1.0648592710494995, 'learning_rate': 1.3462078947635781e-05, 'epoch': 2.08}\n",
            "{'loss': 0.9082, 'grad_norm': 1.2799575328826904, 'learning_rate': 9.518035029974126e-06, 'epoch': 2.24}\n",
            "{'loss': 0.7781, 'grad_norm': 1.099693775177002, 'learning_rate': 6.120266632701599e-06, 'epoch': 2.4}\n",
            "{'loss': 0.7237, 'grad_norm': 1.084578275680542, 'learning_rate': 3.388662781725141e-06, 'epoch': 2.56}\n",
            "{'loss': 0.8858, 'grad_norm': 2.263737440109253, 'learning_rate': 1.419607128479053e-06, 'epoch': 2.72}\n",
            "{'loss': 0.8184, 'grad_norm': 0.8354460597038269, 'learning_rate': 2.825770877317363e-07, 'epoch': 2.88}\n",
            "100% 186/186 [30:20<00:00, 10.48s/it][INFO|trainer.py:2329] 2024-07-15 13:40:09,544 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1821.0143, 'train_samples_per_second': 0.824, 'train_steps_per_second': 0.102, 'train_loss': 1.235791419142036, 'epoch': 2.98}\n",
            "100% 186/186 [30:20<00:00,  9.79s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-15 13:40:09,547 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:733] 2024-07-15 13:40:10,111 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-15 13:40:10,112 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-15 13:40:10,333 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-15 13:40:10,333 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.976\n",
            "  total_flos               = 16111813GF\n",
            "  train_loss               =     1.2358\n",
            "  train_runtime            = 0:30:21.01\n",
            "  train_samples_per_second =      0.824\n",
            "  train_steps_per_second   =      0.102\n",
            "[INFO|modelcard.py:450] 2024-07-15 13:40:10,537 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "9b6e3d2c38a446d59002f128f3ca0ddc",
            "53cddc78da1a4a298a3813d9ce83fbd5",
            "52363b25e32c4c3aafb3221bc58ae761",
            "2877f90bf21040b8a282847c326bdebd",
            "dfbbb6f5b9c84dbeb6fc6e63856b861f",
            "d382e29f336d4fc38d3fc5bfa94b50aa",
            "67adb6d3312c417b8a17c6dc2fdbc4e0",
            "2aef5af4553c48938b42466358587721",
            "0ec0215c203346c58717fb1ac8c86afd",
            "db170ab0f30b4d1e87e810786c44d562",
            "1fc9e42c2e784c089050e1cd985ff0fe",
            "82718ea9c2e04e0aaf7e9d2d87c0e9d0",
            "e59ce7e04513420e9b9fa06460a7b877",
            "d4e0ac7492ab4047a1b8ac57c3907f80",
            "e2d4d1c7f53242f8bd444e7ea86095ee",
            "d9fc23c6f5684d30884cf7a504639402",
            "f56f1d8e285c401a91fc3c768f7a2a6a",
            "1bac30ec377e4430b30e2c72f89aa9a8",
            "8a0b562be51a4d02b442cf9062e47ab9",
            "f43750cfaa7b46dd9f8a31bdfe3c5308",
            "ad48dcd3b3c3481b9060d653bc0774a9",
            "3289a936b30646fcac5ccd3a5e73e7c0",
            "93f307da86544651a1c3bebe5f2cfc3f",
            "091c6f3362824d81be0c99dfa6570bc3",
            "1159112e7c9642f98ae5d78f69e2ac41",
            "fec29a249b8347fc8da8ac9faf83b18b",
            "412c420876524ad6916e7406d60d9e7b",
            "5b8f64329f454fa8b23c54ab9a100a8b",
            "0d39af16d1674cb3a1a6c2395c2f0e9f",
            "42c3db18f08d416ca9ecc75899c34f0a",
            "b8750d30d5924e0488bdbdf4e8bdcdf9",
            "af7d3afbe2724396b68808c62f93045e"
          ]
        },
        "outputId": "1c8f464f-a8e5-4801-dde7-c669c72744a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b6e3d2c38a446d59002f128f3ca0ddc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)"
      ],
      "metadata": {
        "id": "nTZdrPIW1zAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "6Xcs6tC71X3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  export_hub_model_id=\"mshojaei77/Llama3_SFT\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1zdCtu40DSp",
        "outputId": "d44eb727-0df9-400d-d436-ee76af5b35c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-15 14:37:29.448094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-15 14:37:29.448164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-15 14:37:29.450039: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-15 14:37:31.380518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "tokenizer_config.json: 100% 51.0k/51.0k [00:00<00:00, 1.38MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 20.4MB/s]\n",
            "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 519kB/s]\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 14:37:42,066 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 14:37:42,067 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 14:37:42,067 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 14:37:42,067 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-15 14:37:42,462 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/15/2024 14:37:42 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/15/2024 14:37:42 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 654/654 [00:00<00:00, 4.40MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-15 14:37:43,014 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-15 14:37:43,015 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/15/2024 14:37:43 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 78.6MB/s]\n",
            "[INFO|modeling_utils.py:3474] 2024-07-15 14:37:44,299 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/4.98G [00:01<12:47, 6.47MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 21.0M/4.98G [00:02<07:16, 11.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 31.5M/4.98G [00:02<05:34, 14.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 41.9M/4.98G [00:02<04:42, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 52.4M/4.98G [00:03<04:15, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 62.9M/4.98G [00:03<03:57, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 73.4M/4.98G [00:04<03:48, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 83.9M/4.98G [00:05<05:20, 15.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 94.4M/4.98G [00:05<05:05, 16.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 105M/4.98G [00:06<04:35, 17.7MB/s] \u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 115M/4.98G [00:07<04:38, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 126M/4.98G [00:07<04:24, 18.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 136M/4.98G [00:07<04:06, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 147M/4.98G [00:08<03:52, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 157M/4.98G [00:08<03:44, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 168M/4.98G [00:09<03:36, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 178M/4.98G [00:09<03:33, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 189M/4.98G [00:10<03:30, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 199M/4.98G [00:10<03:28, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 210M/4.98G [00:11<03:27, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 220M/4.98G [00:11<03:26, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 231M/4.98G [00:12<03:23, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 241M/4.98G [00:12<03:22, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 252M/4.98G [00:12<03:22, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 262M/4.98G [00:13<03:20, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 273M/4.98G [00:13<03:19, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 283M/4.98G [00:14<03:18, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 294M/4.98G [00:14<03:18, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 304M/4.98G [00:15<04:24, 17.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 315M/4.98G [00:16<04:02, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 325M/4.98G [00:16<03:49, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 336M/4.98G [00:16<03:38, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 346M/4.98G [00:17<03:31, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 357M/4.98G [00:17<03:25, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 367M/4.98G [00:18<03:21, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 377M/4.98G [00:18<03:18, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 388M/4.98G [00:19<03:16, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 398M/4.98G [00:19<03:15, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 409M/4.98G [00:20<03:13, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 419M/4.98G [00:20<03:12, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 430M/4.98G [00:20<03:12, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 440M/4.98G [00:21<03:12, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 451M/4.98G [00:21<03:12, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 461M/4.98G [00:22<03:11, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 472M/4.98G [00:22<03:09, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 482M/4.98G [00:23<03:09, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 493M/4.98G [00:23<03:08, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 503M/4.98G [00:24<03:08, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 514M/4.98G [00:24<03:07, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 524M/4.98G [00:24<03:07, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 535M/4.98G [00:25<03:06, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 545M/4.98G [00:25<03:06, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 556M/4.98G [00:26<03:07, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 566M/4.98G [00:26<03:07, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 577M/4.98G [00:27<03:09, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 587M/4.98G [00:28<04:09, 17.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 598M/4.98G [00:28<03:50, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 608M/4.98G [00:28<03:35, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 619M/4.98G [00:29<03:27, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 629M/4.98G [00:29<03:18, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 640M/4.98G [00:30<03:14, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 650M/4.98G [00:30<03:09, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 661M/4.98G [00:31<03:08, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 671M/4.98G [00:31<03:06, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 682M/4.98G [00:32<03:05, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 692M/4.98G [00:32<03:04, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 703M/4.98G [00:32<03:02, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 713M/4.98G [00:33<03:02, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 724M/4.98G [00:33<03:00, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 734M/4.98G [00:34<03:00, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 744M/4.98G [00:34<02:58, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 755M/4.98G [00:35<02:59, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 765M/4.98G [00:35<02:57, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 776M/4.98G [00:36<02:58, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 786M/4.98G [00:36<02:58, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 797M/4.98G [00:36<02:58, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 807M/4.98G [00:37<02:58, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 818M/4.98G [00:37<02:57, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 828M/4.98G [00:38<02:57, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 839M/4.98G [00:38<03:17, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 849M/4.98G [00:39<03:18, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 860M/4.98G [00:39<03:10, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 870M/4.98G [00:40<03:04, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 881M/4.98G [00:40<03:00, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 891M/4.98G [00:41<02:57, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 902M/4.98G [00:41<02:55, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 912M/4.98G [00:42<02:53, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 923M/4.98G [00:42<02:52, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 933M/4.98G [00:42<02:51, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 944M/4.98G [00:43<02:51, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 954M/4.98G [00:43<02:49, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 965M/4.98G [00:44<02:49, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 975M/4.98G [00:44<02:48, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 986M/4.98G [00:45<02:48, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 996M/4.98G [00:45<02:47, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [00:46<02:48, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.02G/4.98G [00:46<02:54, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.03G/4.98G [00:47<03:10, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.04G/4.98G [00:47<03:03, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.05G/4.98G [00:48<02:57, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.06G/4.98G [00:48<02:53, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.07G/4.98G [00:49<03:46, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.08G/4.98G [00:49<03:28, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.09G/4.98G [00:50<03:14, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.10G/4.98G [00:50<03:05, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.11G/4.98G [00:51<02:58, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.12G/4.98G [00:51<02:54, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.13G/4.98G [00:52<02:51, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:52<02:49, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.15G/4.98G [00:53<02:48, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.16G/4.98G [00:53<02:46, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.17G/4.98G [00:53<02:43, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.18G/4.98G [00:54<02:43, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.20G/4.98G [00:54<02:41, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.21G/4.98G [00:55<02:41, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.22G/4.98G [00:55<02:39, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.23G/4.98G [00:56<02:40, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:56<02:39, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.25G/4.98G [00:57<02:39, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.26G/4.98G [00:57<02:39, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.27G/4.98G [00:57<02:38, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [00:58<02:39, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.29G/4.98G [00:58<02:38, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.30G/4.98G [00:59<02:36, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.31G/4.98G [01:00<03:27, 17.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.32G/4.98G [01:00<03:10, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.33G/4.98G [01:01<02:59, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.34G/4.98G [01:01<02:50, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.35G/4.98G [01:02<02:45, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.36G/4.98G [01:02<02:40, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.37G/4.98G [01:02<02:38, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.38G/4.98G [01:03<02:35, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.39G/4.98G [01:03<02:34, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.41G/4.98G [01:04<02:33, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.42G/4.98G [01:04<02:31, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.43G/4.98G [01:05<02:30, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.44G/4.98G [01:05<02:29, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.45G/4.98G [01:06<02:29, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.46G/4.98G [01:06<02:27, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.47G/4.98G [01:06<02:28, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [01:07<02:26, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.49G/4.98G [01:07<02:27, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.50G/4.98G [01:08<02:25, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.51G/4.98G [01:08<02:26, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.52G/4.98G [01:09<02:24, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.53G/4.98G [01:09<02:40, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.54G/4.98G [01:10<02:37, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.55G/4.98G [01:10<02:33, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.56G/4.98G [01:11<02:31, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.57G/4.98G [01:11<02:29, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.58G/4.98G [01:11<02:27, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.59G/4.98G [01:12<02:26, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.60G/4.98G [01:12<02:28, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.61G/4.98G [01:13<02:26, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.63G/4.98G [01:13<02:25, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.64G/4.98G [01:14<02:24, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.65G/4.98G [01:14<02:23, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.66G/4.98G [01:15<02:21, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.67G/4.98G [01:15<02:21, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [01:16<02:20, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.69G/4.98G [01:16<02:19, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.70G/4.98G [01:16<02:18, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.71G/4.98G [01:17<02:18, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.72G/4.98G [01:17<02:18, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.73G/4.98G [01:18<02:19, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.74G/4.98G [01:18<02:18, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [01:19<02:17, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.76G/4.98G [01:19<02:17, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.77G/4.98G [01:20<03:02, 17.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.78G/4.98G [01:20<02:48, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.79G/4.98G [01:21<02:37, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.80G/4.98G [01:21<02:30, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.81G/4.98G [01:22<02:24, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.82G/4.98G [01:22<02:21, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.84G/4.98G [01:23<02:26, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.85G/4.98G [01:23<02:34, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.86G/4.98G [01:24<02:27, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.87G/4.98G [01:24<02:22, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.88G/4.98G [01:25<02:18, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.89G/4.98G [01:25<02:15, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.90G/4.98G [01:26<02:13, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.91G/4.98G [01:26<02:11, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.92G/4.98G [01:26<02:10, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.93G/4.98G [01:27<02:09, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.94G/4.98G [01:27<02:09, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [01:28<02:08, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [01:28<02:08, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.97G/4.98G [01:29<02:07, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.98G/4.98G [01:29<02:06, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.99G/4.98G [01:30<02:05, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.00G/4.98G [01:30<02:05, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.01G/4.98G [01:30<02:04, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.02G/4.98G [01:31<02:04, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.03G/4.98G [01:31<02:03, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.04G/4.98G [01:32<02:04, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.06G/4.98G [01:32<02:02, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.07G/4.98G [01:33<02:03, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.08G/4.98G [01:34<02:45, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.09G/4.98G [01:34<02:31, 19.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.10G/4.98G [01:35<02:22, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.11G/4.98G [01:35<02:15, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.12G/4.98G [01:35<02:11, 21.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.13G/4.98G [01:36<02:07, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.14G/4.98G [01:36<02:05, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [01:37<02:03, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.16G/4.98G [01:37<02:01, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.17G/4.98G [01:38<02:00, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.18G/4.98G [01:38<02:00, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.19G/4.98G [01:39<02:00, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.20G/4.98G [01:39<01:59, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.21G/4.98G [01:39<01:59, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.22G/4.98G [01:40<01:58, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.23G/4.98G [01:40<01:56, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.24G/4.98G [01:41<01:56, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.25G/4.98G [01:41<01:55, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.26G/4.98G [01:42<01:55, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.28G/4.98G [01:42<01:54, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.29G/4.98G [01:43<01:55, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.30G/4.98G [01:43<01:53, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.31G/4.98G [01:43<01:54, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.32G/4.98G [01:44<01:52, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.33G/4.98G [01:44<01:52, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.34G/4.98G [01:45<01:51, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [01:45<01:51, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.36G/4.98G [01:46<01:50, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.37G/4.98G [01:46<01:50, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.38G/4.98G [01:47<01:48, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.39G/4.98G [01:47<01:49, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.40G/4.98G [01:47<01:47, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.41G/4.98G [01:48<01:48, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.42G/4.98G [01:48<01:46, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.43G/4.98G [01:49<01:50, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.44G/4.98G [01:49<02:02, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.45G/4.98G [01:50<01:58, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.46G/4.98G [01:50<01:53, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.47G/4.98G [01:51<01:51, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.49G/4.98G [01:51<01:48, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.50G/4.98G [01:52<01:47, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.51G/4.98G [01:52<01:45, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.52G/4.98G [01:53<01:44, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.53G/4.98G [01:53<01:43, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.54G/4.98G [01:53<01:43, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.55G/4.98G [01:54<01:42, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.56G/4.98G [01:54<01:43, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.57G/4.98G [01:55<01:43, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.58G/4.98G [01:55<01:42, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.59G/4.98G [01:56<01:42, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.60G/4.98G [01:56<01:41, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.61G/4.98G [01:57<01:40, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [01:57<01:40, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.63G/4.98G [01:57<01:39, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.64G/4.98G [01:58<01:39, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.65G/4.98G [01:58<01:38, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.66G/4.98G [01:59<01:38, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.67G/4.98G [02:00<02:11, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.68G/4.98G [02:00<02:00, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.69G/4.98G [02:01<01:53, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.71G/4.98G [02:01<01:49, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.72G/4.98G [02:02<01:45, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.73G/4.98G [02:02<01:41, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.74G/4.98G [02:02<01:39, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.75G/4.98G [02:03<01:37, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.76G/4.98G [02:03<01:36, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.77G/4.98G [02:04<01:34, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.78G/4.98G [02:04<01:34, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.79G/4.98G [02:05<01:32, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.80G/4.98G [02:05<01:32, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.81G/4.98G [02:06<01:31, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.82G/4.98G [02:06<01:31, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.83G/4.98G [02:06<01:31, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [02:07<01:31, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.85G/4.98G [02:07<01:29, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.86G/4.98G [02:08<01:29, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.87G/4.98G [02:08<01:38, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.88G/4.98G [02:09<01:36, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [02:09<01:33, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.90G/4.98G [02:10<01:31, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.92G/4.98G [02:10<01:29, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.93G/4.98G [02:11<01:28, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.94G/4.98G [02:11<01:27, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.95G/4.98G [02:11<01:26, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.96G/4.98G [02:12<01:26, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.97G/4.98G [02:12<01:24, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.98G/4.98G [02:13<01:24, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.99G/4.98G [02:13<01:23, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.00G/4.98G [02:14<01:23, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.01G/4.98G [02:14<01:22, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.02G/4.98G [02:15<01:22, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.03G/4.98G [02:15<01:22, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.04G/4.98G [02:15<01:22, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.05G/4.98G [02:16<01:22, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.06G/4.98G [02:16<01:22, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.07G/4.98G [02:17<01:22, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.08G/4.98G [02:17<01:21, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [02:18<01:20, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.10G/4.98G [02:18<01:20, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.11G/4.98G [02:19<01:46, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.12G/4.98G [02:20<01:37, 19.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.14G/4.98G [02:20<01:31, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.15G/4.98G [02:20<01:26, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.16G/4.98G [02:21<01:23, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.17G/4.98G [02:21<01:21, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.18G/4.98G [02:22<01:29, 20.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.19G/4.98G [02:22<01:25, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.20G/4.98G [02:23<01:22, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.21G/4.98G [02:23<01:19, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [02:24<01:17, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.23G/4.98G [02:24<01:16, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.24G/4.98G [02:25<01:15, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.25G/4.98G [02:25<01:14, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.26G/4.98G [02:26<01:14, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.27G/4.98G [02:26<01:14, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.28G/4.98G [02:26<01:13, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.29G/4.98G [02:27<01:12, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.30G/4.98G [02:27<01:11, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.31G/4.98G [02:28<01:11, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.32G/4.98G [02:28<01:09, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.33G/4.98G [02:29<01:09, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.34G/4.98G [02:29<01:08, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.36G/4.98G [02:30<01:08, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.37G/4.98G [02:30<01:07, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.38G/4.98G [02:30<01:07, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.39G/4.98G [02:31<01:06, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.40G/4.98G [02:31<01:06, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.41G/4.98G [02:32<01:05, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.42G/4.98G [02:33<01:28, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.43G/4.98G [02:33<01:21, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [02:34<01:16, 20.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.45G/4.98G [02:34<01:12, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.46G/4.98G [02:34<01:09, 21.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.47G/4.98G [02:35<01:07, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.48G/4.98G [02:35<01:05, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.49G/4.98G [02:36<01:04, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.50G/4.98G [02:36<01:03, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.51G/4.98G [02:37<01:02, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.52G/4.98G [02:37<01:02, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.53G/4.98G [02:38<01:01, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.54G/4.98G [02:38<01:01, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.55G/4.98G [02:38<01:00, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.57G/4.98G [02:39<00:59, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.58G/4.98G [02:39<00:59, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.59G/4.98G [02:40<00:58, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.60G/4.98G [02:40<00:58, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.61G/4.98G [02:41<00:57, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.62G/4.98G [02:41<00:58, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.63G/4.98G [02:42<00:57, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.64G/4.98G [02:42<00:56, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.65G/4.98G [02:43<00:56, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.66G/4.98G [02:43<00:55, 23.6MB/s]\u001b[A"
          ]
        }
      ]
    }
  ]
}