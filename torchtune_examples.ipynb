{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hVuhuGEjtBmT"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/Awesome-Fine-tuning/blob/main/torchtune_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ_2M2Prksur"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchao\n",
        "!pip install torchtune"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with LoRA\n",
        "\n",
        "Works! Set `packed=True` (under `dataset`, set `max_seq_len` under `tokenizer`) and `compile=True` (under `# Training`) and it trains in 4 minutes instead of 50!"
      ],
      "metadata": {
        "id": "K22sJ0yZs-y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device # /tmp may be deleted by your system. Change it to your preference.\n",
        "\n",
        "# Model Arguments\n",
        "model:\n",
        "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
        "  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']\n",
        "  apply_lora_to_mlp: False\n",
        "  lora_rank: 4  # higher increases accuracy and memory\n",
        "  lora_alpha: 8  # usually alpha=2*rank\n",
        "  lora_dropout: 0.0\n",
        "\n",
        "tokenizer:\n",
        "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
        "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
        "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
        "  max_seq_len: 8192\n",
        "\n",
        "checkpointer:\n",
        "  _component_: torchtune.training.FullModelHFCheckpointer\n",
        "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
        "  checkpoint_files: [\n",
        "    model.safetensors\n",
        "  ]\n",
        "  recipe_checkpoint: null\n",
        "  output_dir: ${output_dir}\n",
        "  model_type: QWEN2\n",
        "\n",
        "resume_from_checkpoint: False\n",
        "\n",
        "# Dataset and Sampler\n",
        "dataset:\n",
        "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
        "  packed: True  # True increases speed\n",
        "seed: null\n",
        "shuffle: True\n",
        "batch_size: 4\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer:\n",
        "  _component_: torch.optim.AdamW\n",
        "  fused: True\n",
        "  weight_decay: 0.01\n",
        "  lr: 2e-3\n",
        "\n",
        "lr_scheduler:\n",
        "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
        "  num_warmup_steps: 100\n",
        "\n",
        "loss:\n",
        "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
        "\n",
        "# Training\n",
        "epochs: 1\n",
        "max_steps_per_epoch: null\n",
        "gradient_accumulation_steps: 8  # Use to increase effective batch size\n",
        "clip_grad_norm: null\n",
        "compile: True  # torch.compile the model + loss, True increases speed + decreases memory\n",
        "\n",
        "# Logging\n",
        "metric_logger:\n",
        "  _component_: torchtune.training.metric_logging.DiskLogger\n",
        "  log_dir: ${output_dir}/logs\n",
        "log_every_n_steps: 1\n",
        "log_peak_memory_stats: True\n",
        "\n",
        "# Environment\n",
        "device: cuda\n",
        "dtype: bf16\n",
        "\n",
        "# Activations Memory\n",
        "enable_activation_checkpointing: True  # True reduces memory\n",
        "enable_activation_offloading: False  # True reduces memory\n",
        "\n",
        "# Show case the usage of pytorch profiler\n",
        "# Set enabled to False as it's only needed for debugging training\n",
        "profiler:\n",
        "  _component_: torchtune.training.setup_torch_profiler\n",
        "  enabled: False\n",
        "\n",
        "  #Output directory of trace artifacts\n",
        "  output_dir: ${output_dir}/profiling_outputs\n",
        "\n",
        "  #`torch.profiler.ProfilerActivity` types to trace\n",
        "  cpu: True\n",
        "  cuda: True\n",
        "\n",
        "  #trace options passed to `torch.profiler.profile`\n",
        "  profile_memory: False\n",
        "  with_stack: False\n",
        "  record_shapes: True\n",
        "  with_flops: False\n",
        "\n",
        "  # `torch.profiler.schedule` options:\n",
        "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
        "  wait_steps: 5\n",
        "  warmup_steps: 5\n",
        "  active_steps: 2\n",
        "  num_cycles: 1\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'torchtune_test.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "js8_l7JgoUda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7LtgoOvonJa",
        "outputId": "f69274fa-740b-4e57-8edb-e1c716847278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring files matching the following patterns: None\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 8.04MB/s]\n",
            "LICENSE: 100% 11.3k/11.3k [00:00<00:00, 49.0MB/s]\n",
            "README.md: 100% 3.56k/3.56k [00:00<00:00, 26.4MB/s]\n",
            "config.json: 100% 659/659 [00:00<00:00, 4.77MB/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.99MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 1.95MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:03<00:00, 296MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:01<00:00, 5.49MB/s]\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 11.7MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 11.8MB/s]\n",
            "Successfully downloaded model repo and wrote to the following locations:\n",
            "/tmp/Qwen2-0.5B-Instruct/.gitattributes\n",
            "/tmp/Qwen2-0.5B-Instruct/config.json\n",
            "/tmp/Qwen2-0.5B-Instruct/tokenizer_config.json\n",
            "/tmp/Qwen2-0.5B-Instruct/model.safetensors\n",
            "/tmp/Qwen2-0.5B-Instruct/merges.txt\n",
            "/tmp/Qwen2-0.5B-Instruct/original_repo_id.json\n",
            "/tmp/Qwen2-0.5B-Instruct/vocab.json\n",
            "/tmp/Qwen2-0.5B-Instruct/generation_config.json\n",
            "/tmp/Qwen2-0.5B-Instruct/.cache\n",
            "/tmp/Qwen2-0.5B-Instruct/LICENSE\n",
            "/tmp/Qwen2-0.5B-Instruct/README.md\n",
            "/tmp/Qwen2-0.5B-Instruct/tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run lora_finetune_single_device --config ./torchtune_test.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLLnZ9ZypRJD",
        "outputId": "765aa1bb-a4c1-48b8-aab9-bbba8ffe1c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
            "  checkpoint_files:\n",
            "  - model.safetensors\n",
            "  model_type: QWEN2\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "  recipe_checkpoint: null\n",
            "clip_grad_norm: null\n",
            "compile: true\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  packed: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "enable_activation_offloading: false\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 8\n",
            "log_every_n_steps: 1\n",
            "log_peak_memory_stats: true\n",
            "loss:\n",
            "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.training.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/logs\n",
            "model:\n",
            "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
            "  apply_lora_to_mlp: false\n",
            "  lora_alpha: 8\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  - output_proj\n",
            "  lora_dropout: 0.0\n",
            "  lora_rank: 4\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  fused: true\n",
            "  lr: 0.002\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "profiler:\n",
            "  _component_: torchtune.training.setup_torch_profiler\n",
            "  active_steps: 2\n",
            "  cpu: true\n",
            "  cuda: true\n",
            "  enabled: false\n",
            "  num_cycles: 1\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/profiling_outputs\n",
            "  profile_memory: false\n",
            "  record_shapes: true\n",
            "  wait_steps: 5\n",
            "  warmup_steps: 5\n",
            "  with_flops: false\n",
            "  with_stack: false\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
            "  max_seq_len: 8192\n",
            "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
            "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 481519447. Local seed is seed + rank = 481519447 + 0\n",
            "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
            "Writing logs to /tmp/torchtune/qwen2_0_5B/lora_single_device/logs/log_1737322360.txt\n",
            "INFO:torchtune.utils._logging:Compiling model layers with torch.compile...\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Memory stats after model init:\n",
            "\tGPU peak memory allocation: 1.35 GiB\n",
            "\tGPU peak memory reserved: 1.40 GiB\n",
            "\tGPU peak memory active: 1.35 GiB\n",
            "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils._logging:Compiling loss with torch.compile...\n",
            "INFO:torchtune.utils._logging:Loss is initialized.\n",
            "Packing dataset: 100% 51760/51760 [00:44<00:00, 1152.39it/s]\n",
            "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
            "WARNING:torchtune.utils._logging: Profiling disabled.\n",
            "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
            "INFO:torchtune.utils._logging:NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration.\n",
            "  0% 0/39 [00:00<?, ?it/s]DEBUG:torchtune.utils._logging:Using flex attention for attention computation since a BlockMask was passed in.\n",
            "1|39|Loss: 1.2373169660568237: 100% 39/39 [04:06<00:00,  5.64s/it]INFO:torchtune.utils._logging:Starting checkpoint save...\n",
            "INFO:torchtune.utils._logging:Model checkpoint of size 0.92 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/ft-model-00001-of-00001.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_model.pt\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_model.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_config.json\n",
            "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
            "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
            "INFO:torchtune.utils._logging:Checkpoint saved in 2.62 seconds.\n",
            "1|39|Loss: 1.2373169660568237: 100% 39/39 [04:10<00:00,  6.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with rsLoRA\n",
        "\n",
        "Doesn't train---they don't support rsLoRA.\n",
        "\n",
        "```\n",
        "TypeError: lora_qwen2_0_5b() got an unexpected keyword argument 'use_rslora'\n",
        "```\n",
        "\n",
        "```python\n",
        "def lora_qwen2_0_5b(\n",
        "    lora_attn_modules: List[LORA_ATTN_MODULES],\n",
        "    apply_lora_to_mlp: bool = False,\n",
        "    lora_rank: int = 8,\n",
        "    lora_alpha: float = 16,\n",
        "    lora_dropout: float = 0.0,\n",
        "    use_dora: bool = False,\n",
        "    quantize_base: bool = False,\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "hVuhuGEjtBmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device # /tmp may be deleted by your system. Change it to your preference.\n",
        "\n",
        "# Model Arguments\n",
        "model:\n",
        "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
        "  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']\n",
        "  apply_lora_to_mlp: False\n",
        "  lora_rank: 4  # higher increases accuracy and memory\n",
        "  lora_alpha: 8  # usually alpha=2*rank\n",
        "  lora_dropout: 0.0\n",
        "  use_rslora: True\n",
        "\n",
        "tokenizer:\n",
        "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
        "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
        "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
        "  max_seq_len: null\n",
        "\n",
        "checkpointer:\n",
        "  _component_: torchtune.training.FullModelHFCheckpointer\n",
        "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
        "  checkpoint_files: [\n",
        "    model.safetensors\n",
        "  ]\n",
        "  recipe_checkpoint: null\n",
        "  output_dir: ${output_dir}\n",
        "  model_type: QWEN2\n",
        "\n",
        "resume_from_checkpoint: False\n",
        "\n",
        "# Dataset and Sampler\n",
        "dataset:\n",
        "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
        "  packed: False  # True increases speed\n",
        "seed: null\n",
        "shuffle: True\n",
        "batch_size: 4\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer:\n",
        "  _component_: torch.optim.AdamW\n",
        "  fused: True\n",
        "  weight_decay: 0.01\n",
        "  lr: 2e-3\n",
        "\n",
        "lr_scheduler:\n",
        "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
        "  num_warmup_steps: 100\n",
        "\n",
        "loss:\n",
        "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
        "\n",
        "# Training\n",
        "epochs: 1\n",
        "max_steps_per_epoch: null\n",
        "gradient_accumulation_steps: 8  # Use to increase effective batch size\n",
        "clip_grad_norm: null\n",
        "compile: False  # torch.compile the model + loss, True increases speed + decreases memory\n",
        "\n",
        "# Logging\n",
        "metric_logger:\n",
        "  _component_: torchtune.training.metric_logging.DiskLogger\n",
        "  log_dir: ${output_dir}/logs\n",
        "log_every_n_steps: 1\n",
        "log_peak_memory_stats: True\n",
        "\n",
        "# Environment\n",
        "device: cuda\n",
        "dtype: bf16\n",
        "\n",
        "# Activations Memory\n",
        "enable_activation_checkpointing: True  # True reduces memory\n",
        "enable_activation_offloading: False  # True reduces memory\n",
        "\n",
        "# Show case the usage of pytorch profiler\n",
        "# Set enabled to False as it's only needed for debugging training\n",
        "profiler:\n",
        "  _component_: torchtune.training.setup_torch_profiler\n",
        "  enabled: False\n",
        "\n",
        "  #Output directory of trace artifacts\n",
        "  output_dir: ${output_dir}/profiling_outputs\n",
        "\n",
        "  #`torch.profiler.ProfilerActivity` types to trace\n",
        "  cpu: True\n",
        "  cuda: True\n",
        "\n",
        "  #trace options passed to `torch.profiler.profile`\n",
        "  profile_memory: False\n",
        "  with_stack: False\n",
        "  record_shapes: True\n",
        "  with_flops: False\n",
        "\n",
        "  # `torch.profiler.schedule` options:\n",
        "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
        "  wait_steps: 5\n",
        "  warmup_steps: 5\n",
        "  active_steps: 2\n",
        "  num_cycles: 1\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'torchtune_test.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "oGVJjZDCs0dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run lora_finetune_single_device --config ./torchtune_test.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyeRUDhMtKDM",
        "outputId": "0b7087af-e59c-4308-ed70-467646956d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
            "  checkpoint_files:\n",
            "  - model.safetensors\n",
            "  model_type: QWEN2\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "  recipe_checkpoint: null\n",
            "clip_grad_norm: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  packed: false\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "enable_activation_offloading: false\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 8\n",
            "log_every_n_steps: 1\n",
            "log_peak_memory_stats: true\n",
            "loss:\n",
            "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.training.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/logs\n",
            "model:\n",
            "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
            "  apply_lora_to_mlp: false\n",
            "  lora_alpha: 8\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  - output_proj\n",
            "  lora_dropout: 0.0\n",
            "  lora_rank: 4\n",
            "  use_rslora: true\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  fused: true\n",
            "  lr: 0.002\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "profiler:\n",
            "  _component_: torchtune.training.setup_torch_profiler\n",
            "  active_steps: 2\n",
            "  cpu: true\n",
            "  cuda: true\n",
            "  enabled: false\n",
            "  num_cycles: 1\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/profiling_outputs\n",
            "  profile_memory: false\n",
            "  record_shapes: true\n",
            "  wait_steps: 5\n",
            "  warmup_steps: 5\n",
            "  with_flops: false\n",
            "  with_stack: false\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
            "  max_seq_len: null\n",
            "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
            "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 3604465268. Local seed is seed + rank = 3604465268 + 0\n",
            "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
            "Writing logs to /tmp/torchtune/qwen2_0_5B/lora_single_device/logs/log_1737309339.txt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tune\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/tune.py\", line 49, in main\n",
            "    parser.run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/tune.py\", line 43, in run\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
            "    self._run_single_device(args, is_builtin=is_builtin)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
            "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
            "  File \"<frozen runpy>\", line 291, in run_path\n",
            "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 803, in <module>\n",
            "    sys.exit(recipe_main())\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
            "    sys.exit(recipe_main(conf))\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 797, in recipe_main\n",
            "    recipe.setup(cfg=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 267, in setup\n",
            "    self._model = self._setup_model(\n",
            "                  ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 424, in _setup_model\n",
            "    model = config.instantiate(cfg_model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/config/_instantiate.py\", line 112, in instantiate\n",
            "    return _instantiate_node(OmegaConf.to_object(config), *args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/config/_instantiate.py\", line 33, in _instantiate_node\n",
            "    return _create_component(_component_, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/config/_instantiate.py\", line 22, in _create_component\n",
            "    return _component_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: lora_qwen2_0_5b() got an unexpected keyword argument 'use_rslora'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with DoRA\n",
        "\n",
        "Works but says it will take 50 minutes on an A100 which is odd."
      ],
      "metadata": {
        "id": "SUSmTvWFtQ6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device # /tmp may be deleted by your system. Change it to your preference.\n",
        "\n",
        "# Model Arguments\n",
        "model:\n",
        "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
        "  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']\n",
        "  apply_lora_to_mlp: False\n",
        "  lora_rank: 4  # higher increases accuracy and memory\n",
        "  lora_alpha: 8  # usually alpha=2*rank\n",
        "  lora_dropout: 0.0\n",
        "  use_dora: True\n",
        "\n",
        "tokenizer:\n",
        "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
        "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
        "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
        "  max_seq_len: 8192\n",
        "\n",
        "checkpointer:\n",
        "  _component_: torchtune.training.FullModelHFCheckpointer\n",
        "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
        "  checkpoint_files: [\n",
        "    model.safetensors\n",
        "  ]\n",
        "  recipe_checkpoint: null\n",
        "  output_dir: ${output_dir}\n",
        "  model_type: QWEN2\n",
        "\n",
        "resume_from_checkpoint: False\n",
        "\n",
        "# Dataset and Sampler\n",
        "dataset:\n",
        "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
        "  packed: True  # True increases speed\n",
        "seed: null\n",
        "shuffle: True\n",
        "batch_size: 4\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer:\n",
        "  _component_: torch.optim.AdamW\n",
        "  fused: True\n",
        "  weight_decay: 0.01\n",
        "  lr: 2e-3\n",
        "\n",
        "lr_scheduler:\n",
        "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
        "  num_warmup_steps: 100\n",
        "\n",
        "loss:\n",
        "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
        "\n",
        "# Training\n",
        "epochs: 1\n",
        "max_steps_per_epoch: null\n",
        "gradient_accumulation_steps: 8  # Use to increase effective batch size\n",
        "clip_grad_norm: null\n",
        "compile: True  # torch.compile the model + loss, True increases speed + decreases memory\n",
        "\n",
        "# Logging\n",
        "metric_logger:\n",
        "  _component_: torchtune.training.metric_logging.DiskLogger\n",
        "  log_dir: ${output_dir}/logs\n",
        "log_every_n_steps: 1\n",
        "log_peak_memory_stats: True\n",
        "\n",
        "# Environment\n",
        "device: cuda\n",
        "dtype: bf16\n",
        "\n",
        "# Activations Memory\n",
        "enable_activation_checkpointing: True  # True reduces memory\n",
        "enable_activation_offloading: False  # True reduces memory\n",
        "\n",
        "# Show case the usage of pytorch profiler\n",
        "# Set enabled to False as it's only needed for debugging training\n",
        "profiler:\n",
        "  _component_: torchtune.training.setup_torch_profiler\n",
        "  enabled: False\n",
        "\n",
        "  #Output directory of trace artifacts\n",
        "  output_dir: ${output_dir}/profiling_outputs\n",
        "\n",
        "  #`torch.profiler.ProfilerActivity` types to trace\n",
        "  cpu: True\n",
        "  cuda: True\n",
        "\n",
        "  #trace options passed to `torch.profiler.profile`\n",
        "  profile_memory: False\n",
        "  with_stack: False\n",
        "  record_shapes: True\n",
        "  with_flops: False\n",
        "\n",
        "  # `torch.profiler.schedule` options:\n",
        "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
        "  wait_steps: 5\n",
        "  warmup_steps: 5\n",
        "  active_steps: 2\n",
        "  num_cycles: 1\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'torchtune_test.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "V3J1OfgFtKvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run lora_finetune_single_device --config ./torchtune_test.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U326DaZt3d6",
        "outputId": "fcce98ac-b322-4cd9-894e-0a2e590927cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct\n",
            "  checkpoint_files:\n",
            "  - model.safetensors\n",
            "  model_type: QWEN2\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "  recipe_checkpoint: null\n",
            "clip_grad_norm: null\n",
            "compile: true\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  packed: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "enable_activation_offloading: false\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 8\n",
            "log_every_n_steps: 1\n",
            "log_peak_memory_stats: true\n",
            "loss:\n",
            "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.training.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/logs\n",
            "model:\n",
            "  _component_: torchtune.models.qwen2.lora_qwen2_0_5b\n",
            "  apply_lora_to_mlp: false\n",
            "  lora_alpha: 8\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  - output_proj\n",
            "  lora_dropout: 0.0\n",
            "  lora_rank: 4\n",
            "  use_dora: true\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  fused: true\n",
            "  lr: 0.002\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device\n",
            "profiler:\n",
            "  _component_: torchtune.training.setup_torch_profiler\n",
            "  active_steps: 2\n",
            "  cpu: true\n",
            "  cuda: true\n",
            "  enabled: false\n",
            "  num_cycles: 1\n",
            "  output_dir: /tmp/torchtune/qwen2_0_5B/lora_single_device/profiling_outputs\n",
            "  profile_memory: false\n",
            "  record_shapes: true\n",
            "  wait_steps: 5\n",
            "  warmup_steps: 5\n",
            "  with_flops: false\n",
            "  with_stack: false\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
            "  max_seq_len: 8192\n",
            "  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt\n",
            "  path: /tmp/Qwen2-0.5B-Instruct/vocab.json\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 3208054156. Local seed is seed + rank = 3208054156 + 0\n",
            "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
            "Writing logs to /tmp/torchtune/qwen2_0_5B/lora_single_device/logs/log_1737322789.txt\n",
            "INFO:torchtune.utils._logging:Compiling model layers with torch.compile...\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Memory stats after model init:\n",
            "\tGPU peak memory allocation: 1.35 GiB\n",
            "\tGPU peak memory reserved: 1.40 GiB\n",
            "\tGPU peak memory active: 1.35 GiB\n",
            "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils._logging:Compiling loss with torch.compile...\n",
            "INFO:torchtune.utils._logging:Loss is initialized.\n",
            "Packing dataset: 100% 51760/51760 [00:43<00:00, 1184.61it/s]\n",
            "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
            "WARNING:torchtune.utils._logging: Profiling disabled.\n",
            "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
            "INFO:torchtune.utils._logging:NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration.\n",
            "  0% 0/39 [00:00<?, ?it/s]DEBUG:torchtune.utils._logging:Using flex attention for attention computation since a BlockMask was passed in.\n",
            "1|39|Loss: 1.2368882894515991: 100% 39/39 [04:01<00:00,  5.73s/it]INFO:torchtune.utils._logging:Starting checkpoint save...\n",
            "INFO:torchtune.utils._logging:Model checkpoint of size 0.92 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/ft-model-00001-of-00001.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_model.pt\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_model.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/qwen2_0_5B/lora_single_device/epoch_0/adapter_config.json\n",
            "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
            "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
            "INFO:torchtune.utils._logging:Checkpoint saved in 3.33 seconds.\n",
            "1|39|Loss: 1.2368882894515991: 100% 39/39 [04:06<00:00,  6.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xFTmVCKt39S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}